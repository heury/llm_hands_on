{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b805a23",
   "metadata": {},
   "source": [
    "## Sinusoidal 위치 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0bd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. 환경 설정 및 하이퍼파라미터 정의\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "max_length = 4\n",
    "\n",
    "# 2. 토큰 임베딩 층 (기존과 동일)\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# 3. 데이터 로더 및 샘플 추출 (기존과 동일)\n",
    "# dataloader = create_dataloader_v1(...) \n",
    "# inputs, targets = next(iter(dataloader))\n",
    "# 예시를 위해 가상의 inputs 생성 (8개 배치, 4개 단어)\n",
    "inputs = torch.randint(0, vocab_size, (8, max_length))\n",
    "\n",
    "# 4. 토큰 임베딩 수행\n",
    "token_embeddings = token_embedding_layer(inputs) # [8, 4, 256]\n",
    "\n",
    "# 5. 삼각함수 기반 위치 인코딩 행렬 생성 (핵심 변경 부분)\n",
    "def get_sinusoidal_encoding(seq_len, d_model):\n",
    "    # 각 차원에 적용될 주파수(지수 부분) 계산\n",
    "    # 10000^(2i/d_model)\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "    \n",
    "    # 위치 인덱스 (0, 1, 2, 3) 생성\n",
    "    pos = torch.arange(seq_len).float().unsqueeze(1) # [seq_len, 1]\n",
    "    \n",
    "    # 사인/코사인 내부 값 계산\n",
    "    # [seq_len, 1] * [1, d_model/2] -> [seq_len, d_model/2]\n",
    "    sin_cos_args = pos * inv_freq\n",
    "    \n",
    "    # 최종 행렬 생성\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    pe[:, 0::2] = torch.sin(sin_cos_args) # 짝수 열은 Sine\n",
    "    pe[:, 1::2] = torch.cos(sin_cos_args) # 홀수 열은 Cosine\n",
    "    return pe\n",
    "\n",
    "# 위치 인코딩 행렬 생성\n",
    "pos_encodings = get_sinusoidal_encoding(max_length, output_dim)\n",
    "\n",
    "# 6. 생성된 위치 인코딩 확인\n",
    "print(f\"Position Encoding Shape: {pos_encodings.shape}\") # [4, 256]\n",
    "print(\"Position Encoding Values (First 2 rows):\\n\", pos_encodings[:2, :10])\n",
    "\n",
    "# 7. 최종 입력 임베딩 생성 (토큰 정보 + 위치 정보)\n",
    "# 학습되는 파라미터가 아니므로 가중치 업데이트가 필요 없음을 명시하기 위해 .detach()를 쓰기도 합니다.\n",
    "input_embeddings = token_embeddings + pos_encodings\n",
    "print(f\"Final Input Shape: {input_embeddings.shape}\") # [8, 4, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05de079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_hands_on (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

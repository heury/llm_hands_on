{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bece3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        \"\"\"\n",
    "        LoRA(Low-Rank Adaptation) 레이어 초기화\n",
    "        \n",
    "        Args:\n",
    "            in_dim: 입력 데이터의 차원 크기 (예: 768)\n",
    "            out_dim: 출력 데이터의 차원 크기 (예: 768)\n",
    "            rank: 병목 구간의 크기 (Rank, r). 작을수록 파라미터가 적어짐 (예: 8, 16)\n",
    "            alpha: 스케일링 상수. 학습된 값의 영향력을 조절함\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. 행렬 A (Down-projection)\n",
    "        # 입력(in_dim)을 작은 차원(rank)으로 압축하는 가중치입니다.\n",
    "        # 파라미터로 등록하여 학습되도록 합니다.\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        \n",
    "        # A는 무작위 값(Kaiming Uniform)으로 초기화합니다.\n",
    "        # 이렇게 해야 다양한 특징을 학습할 준비가 됩니다.\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        \n",
    "        # 2. 행렬 B (Up-projection)\n",
    "        # 압축된 정보(rank)를 다시 원래 출력 차원(out_dim)으로 복원하는 가중치입니다.\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        \n",
    "        # 중요: B는 '0'으로 초기화합니다.\n",
    "        # 이유: 학습 시작 시점에는 A @ B의 결과가 0이 되어야 합니다.\n",
    "        # 그래야 LoRA를 붙여도 기존 모델의 원래 출력값에 아무런 영향을 주지 않은 채로 시작할 수 있습니다.\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.rank = rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파(Forward) 단계: 입력 x에 대한 LoRA 변화량(Delta)을 계산\n",
    "        \"\"\"\n",
    "        # 1. 행렬 연산 (x @ A @ B)\n",
    "        # 입력 x를 A와 곱해 차원을 줄이고(압축), \n",
    "        # 그 결과를 다시 B와 곱해 차원을 늘립니다(복원).\n",
    "        # 연산 순서: (Batch, in) -> (Batch, rank) -> (Batch, out)\n",
    "        \n",
    "        # 2. 스케일링 (alpha / rank)\n",
    "        # 학습된 결과에 상수배를 해줍니다. \n",
    "        # alpha는 강도 조절, rank로 나누는 것은 rank값이 바뀌어도 \n",
    "        # 학습률(Learning Rate)을 크게 수정하지 않기 위한 정규화(Normalization) 과정입니다.\n",
    "        x = (self.alpha / self.rank) * (x @ self.A @ self.B)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b485f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    기존의 Linear 레이어를 감싸서(Wrapping), \n",
    "    LoRA 어댑터를 추가한 새로운 복합 레이어를 만드는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. 기존 레이어 저장 (Frozen Weights)\n",
    "        # 이미 학습된 레이어 가중치(W)를 그대로 가져옵니다.\n",
    "        # 실제 사용 시에는 이 레이어의 가중치를 고정(freeze)시켜 학습되지 않게 합니다.\n",
    "        self.linear = linear\n",
    "        \n",
    "        # 2. LoRA 레이어 생성 (Trainable Adapter)\n",
    "        # 기존 레이어와 똑같은 입/출력 차원을 가지지만, 내부는 훨씬 가벼운 LoRA 레이어를 만듭니다.\n",
    "        # linear.in_features, linear.out_features: 기존 레이어의 스펙을 그대로 베껴옵니다.\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 3. 결과 합치기 (The Core Logic)\n",
    "        # 원래 모델이 하던 계산 결과 : self.linear(x)\n",
    "        # LoRA가 새로 학습한 변화량 : self.lora(x)\n",
    "        # 최종 결과 = 원래 결과 + 변화량\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc6b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    \"\"\"\n",
    "    모델 내의 모든 Linear 레이어를 찾아 LoRA가 적용된 레이어로 교체하는 재귀 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 수정할 파이토치 모델 (또는 모델의 하위 모듈)\n",
    "        rank: LoRA의 Rank (r)\n",
    "        alpha: LoRA의 Alpha (스케일링 계수)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 자식 모듈 순회 (Iterate Children)\n",
    "    # named_children(): 현재 모듈 바로 아래에 있는 구성 요소들을 (이름, 객체) 쌍으로 가져옵니다.\n",
    "    # 예: (\"attn\", AttentionModule), (\"mlp\", MLPModule) ...\n",
    "    for name, module in model.named_children():\n",
    "        \n",
    "        # 2. 교체 대상 확인 (Check Target)\n",
    "        # 현재 보고 있는 모듈이 'torch.nn.Linear'인지 확인합니다.\n",
    "        # (즉, 우리가 LoRA를 붙이고 싶은 기본 선형 층인지 확인)\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            \n",
    "            # 3. 레이어 교체 (Swap Layer)\n",
    "            # setattr(객체, 속성이름, 새로운값): 객체의 속성을 동적으로 변경하는 파이썬 내장 함수\n",
    "            # model의 'name'에 해당하는 레이어를 -> 'LinearWithLoRA'로 덮어씌웁니다.\n",
    "            # 이때 기존 module을 인자로 넘겨줘서 가중치를 그대로 보존하게 합니다.\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "            \n",
    "        else:\n",
    "            # 4. 재귀 호출 (Dig Deeper)\n",
    "            # 만약 Linear 레이어가 아니라면(예: 트랜스포머 블록, 어텐션 층 등 컨테이너라면),\n",
    "            # 그 내부에도 Linear 레이어가 숨어있을 수 있으므로\n",
    "            # 그 안으로 들어가서 다시 똑같은 작업(replace_linear_with_lora)을 수행하라고 시킵니다.\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f60c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미 파일이 존재합니다: models/gpt2\\gpt2-small-124M.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from previous_chapters import load_gpt2_model\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 어휘사전 크기\n",
    "    \"context_length\": 1024,  # 문맥 길이\n",
    "    \"drop_rate\": 0.0,        # 드롭아웃 비율\n",
    "    \"qkv_bias\": True         # 쿼리-키-값 편향\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12}, #트랜스포머 블록(Transformer Block)을 몇 층으로 쌓아 올렸는지\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_name = \"gpt2-small-124M.pth\"\n",
    "model = load_gpt2_model(model_name, BASE_CONFIG)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d88f7c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전의 총 학습 가능 파라미터 수: 163,037,184\n",
      "이후의 총 학습 가능 파라미터 수: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. 현재 학습 가능한(Trainable) 파라미터 개수 계산\n",
    "# - model.parameters(): 모델 내부의 모든 가중치(W)와 편향(b)을 가져옵니다.\n",
    "# - p.numel(): 각 파라미터 텐서의 전체 원소 개수(Number of Elements)를 셉니다.\n",
    "# - if p.requires_grad: 현재 '학습 대상(True)'으로 설정된 것들만 필터링합니다.\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# {total_params:,} : 천 단위마다 쉼표(,)를 찍어서 보기 좋게 출력 (예: 1,000,000)\n",
    "print(f\"이전의 총 학습 가능 파라미터 수: {total_params:,}\")\n",
    "\n",
    "\n",
    "# 2. 모든 파라미터 얼리기 (Freezing)\n",
    "# 모델의 모든 파라미터를 하나씩 꺼내서 반복합니다.\n",
    "for param in model.parameters():\n",
    "    # requires_grad = False:\n",
    "    # \"이 파라미터는 이제 학습시키지 마(기울기 계산 X)\"라고 설정합니다.\n",
    "    # 이렇게 하면 역전파(Backpropagation) 때 이 값들은 업데이트되지 않고 고정됩니다.\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# 3. 얼린 후 학습 가능 파라미터 수 재계산\n",
    "# 위에서 모든 param의 requires_grad를 False로 바꿨으므로, \n",
    "# 조건(if p.requires_grad)을 만족하는 파라미터가 하나도 없게 됩니다.\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# 결과적으로 0이 출력되어야 정상입니다.\n",
    "print(f\"이후의 총 학습 가능 파라미터 수: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a9bf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 학습 가능 LoRA 파라미터 수: 3,470,608\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"총 학습 가능 LoRA 파라미터 수: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a5d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-hands-on",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

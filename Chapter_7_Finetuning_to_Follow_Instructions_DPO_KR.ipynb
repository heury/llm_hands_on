{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62129596-d10f-45b1-a1af-ee10f358f773",
   "metadata": {
    "id": "62129596-d10f-45b1-a1af-ee10f358f773"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "<a href=\"https://sebastianraschka.com\">Sebastian Raschka</a>의 저서 <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a>를 위한 보충 코드<br>\n",
    "<br>코드 저장소: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"images/llm_from_scratch/dpo/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd2379-ed2f-4c77-8b71-f1f0242b9ff9",
   "metadata": {
    "id": "b0bd2379-ed2f-4c77-8b71-f1f0242b9ff9"
   },
   "source": [
    "# LLM 정렬을 위한 DPO(Direct Preference Optimization) 구현 (밑바닥부터 시작하기)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04cb2b8-d87b-4c6b-a225-c630d758f68e",
   "metadata": {
    "id": "d04cb2b8-d87b-4c6b-a225-c630d758f68e"
   },
   "source": [
    "- 이 코드 노트북은 DPO(Direct Preference Optimization)를 처음부터 구현하고 이를 대규모 언어 모델(LLM)에 적용하여 사용자 선호도에 더 부합하는 응답을 생성하도록 모델 성능을 향상시키는 방법을 다룹니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb3e145-fbaa-4bb3-9e95-186b4145087f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edb3e145-fbaa-4bb3-9e95-186b4145087f",
    "outputId": "3d449525-76cc-4124-ab30-a93c6a9623ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n",
      "torch version: 2.9.1+cu126\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"tiktoken\",    # 토크나이저 (Tokenizer)\n",
    "    \"torch\",       # 딥러닝 라이브러리 (Deep learning library)\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec20a3-a26c-4f9b-8a33-bfd3d67860e2",
   "metadata": {
    "id": "49ec20a3-a26c-4f9b-8a33-bfd3d67860e2"
   },
   "source": [
    "&nbsp;\n",
    "# 1) DPO에 대한 간략한 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17804afd-786b-4600-bad0-f5805454e3d6",
   "metadata": {
    "id": "17804afd-786b-4600-bad0-f5805454e3d6"
   },
   "source": [
    "- 논문 [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)에서 제안된 DPO는 LLM 미세 튜닝(finetuning)에 사용되는 인간 피드백 기반 강화 학습(RLHF)의 대안입니다.\n",
    "- DPO는 모델을 미세 튜닝(또는 정렬, align)하여 사용자의 기대와 지시에 더 잘 맞는 응답을 생성하도록 하는 데 사용될 수 있습니다.\n",
    "\n",
    "<img src=\"images/llm_from_scratch/dpo/1.webp\" width=500px>\n",
    "\n",
    "- 지시 튜닝(Instruction finetuning)에서는 프롬프트가 주어졌을 때 올바른 답변을 생성하도록 LLM을 훈련합니다.\n",
    "- 하지만 실제로는 정답을 제시하는 방법이 여러 가지일 수 있으며, 정답의 스타일도 다를 수 있습니다. 예를 들어, 아래 그림처럼 LLM에게 노트북 구매 추천을 요청할 때 기술적인 답변과 사용자 친화적인 답변을 비교해 볼 수 있습니다.\n",
    "\n",
    "<img src=\"images/llm_from_scratch/dpo/2.webp\" width=700px>\n",
    "\n",
    "- RLHF와 DPO는 LLM이 특정 답변 스타일을 다른 스타일보다 선호하도록, 즉 사용자 선호도에 더 잘 맞추도록 가르치는 데 사용할 수 있는 방법입니다.\n",
    "- 별도의 보상 모델(reward model)을 훈련해야 하는 RLHF 프로세스는 아래와 같습니다.\n",
    "\n",
    "<img src=\"images/llm_from_scratch/dpo/4.webp\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9073622f-d537-42bf-8778-43c2adaa2191",
   "metadata": {
    "id": "9073622f-d537-42bf-8778-43c2adaa2191"
   },
   "source": [
    "- RLHF와 비교했을 때, DPO는 복잡한 보상 모델링이나 정책 최적화(policy optimization) 과정 없이 사용자 선호도에 맞춰 모델을 직접 최적화함으로써 프로세스를 단순화하는 것을 목표로 합니다.\n",
    "- 다시 말해, DPO는 인간의 선호도나 특정 목표에 부합하도록 모델의 출력을 직접 최적화하는 데 중점을 둡니다.\n",
    "- 아래 그림은 DPO의 작동 원리에 대한 개요를 보여줍니다.\n",
    "\n",
    "<img src=\"images/llm_from_scratch/dpo/5.webp\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c894134a-315c-453e-bbc1-387794b3f4d6",
   "metadata": {
    "id": "c894134a-315c-453e-bbc1-387794b3f4d6"
   },
   "source": [
    "- DPO 손실(loss)을 구현하기 위한 구체적인 수식은 아래와 같습니다. 이 수식은 이 노트북의 뒷부분에서 Python으로 구현할 때 다시 살펴보겠습니다.\n",
    "\n",
    "<img src=\"images/llm_from_scratch/dpo/3.webp\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7491b5-f619-4501-ad39-2942de57c115",
   "metadata": {
    "id": "dd7491b5-f619-4501-ad39-2942de57c115"
   },
   "source": [
    "- 위 식에서,\n",
    "  - \"기댓값(expected value)\" $\\mathbb{E}$는 통계 용어로 확률 변수(괄호 안의 표현식)의 평균값을 의미합니다. $-\\mathbb{E}$를 최적화하면 모델이 사용자 선호도에 더 잘 부합하게 됩니다.\n",
    "  - $\\pi_{\\theta}$ 변수는 소위 정책(policy, 강화 학습에서 차용한 용어)이라 불리며 우리가 최적화하고자 하는 LLM을 나타냅니다. $\\pi_{ref}$는 참조(reference) LLM으로, 일반적으로 최적화 전의 원본 LLM입니다 (훈련 시작 시점에는 $\\pi_{\\theta}$와 $\\pi_{ref}$가 보통 동일합니다).\n",
    "  - $\\beta$는 $\\pi_{\\theta}$와 참조 모델 간의 발산(divergence)을 제어하는 하이퍼파라미터입니다. $\\beta$를 높이면 전체 손실 함수에서 $\\pi_{\\theta}$와 $\\pi_{ref}$의 로그 확률 차이가 미치는 영향이 줄어들어, 두 모델 간의 차이(divergence)가 감소합니다.\n",
    "  - 로지스틱 시그모이드 함수 $\\sigma(\\centerdot)$는 선호되는 응답과 거부되는 응답의 로그 오즈(log-odds, 시그모이드 함수 내부의 항)를 확률 점수로 변환합니다.\n",
    "- 코드 노트북이 너무 방대해지는 것을 피하기 위해, 이 개념들에 대한 더 자세한 논의는 향후 별도의 기사로 작성할 예정입니다.\n",
    "- 그동안 RLHF와 DPO의 비교에 관심이 있다면, 제 기사 [Tips for LLM Pretraining and Evaluating Reward Models](https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms)의 [2.2. RLHF vs Direct Preference Optimization (DPO)](https://magazine.sebastianraschka.com/i/142924793/rlhf-vs-direct-preference-optimization-dpo) 섹션을 참조하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xqVAgsyQ6LuG",
   "metadata": {
    "id": "xqVAgsyQ6LuG",
    "tags": []
   },
   "source": [
    "&nbsp;\n",
    "# 2) DPO를 위한 선호도 데이터셋 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2195d-8734-469b-a52e-5031ca7ea6b1",
   "metadata": {
    "id": "60b2195d-8734-469b-a52e-5031ca7ea6b1"
   },
   "source": [
    "- 데이터셋을 로드하고 준비하는 것으로 시작해 보겠습니다. 이 과정을 통해 DPO 손실 수식을 다시 살펴보기 전에 가질 수 있는 많은 질문이 해결될 것입니다.\n",
    "- 여기서 우리는 지시 프롬프트에 대해 더 정중한 응답과 덜 정중한 응답이 포함된 데이터셋을 사용합니다(구체적인 예시는 다음 섹션에 나와 있습니다).\n",
    "- 이 데이터셋은 [create-preference-data-ollama.ipynb](create-preference-data-ollama.ipynb) 노트북을 통해 생성되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wHLB62Nj7haD",
   "metadata": {
    "id": "wHLB62Nj7haD"
   },
   "source": [
    "&nbsp;\n",
    "## 2.1) 선호도 데이터셋 로드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e09f99-1b18-4923-ba36-af46d8e3075f",
   "metadata": {
    "id": "13e09f99-1b18-4923-ba36-af46d8e3075f"
   },
   "source": [
    "- 이 데이터셋은 1,100개의 항목이 있는 json 파일입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5266e66c-5ec0-45e6-a654-148971f6aee7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5266e66c-5ec0-45e6-a654-148971f6aee7",
    "outputId": "04e8ee70-3076-441d-d2bf-7641da3d0c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        text_data = response.text\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    data = json.loads(text_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"datas/instruction-data-with-preference.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/04_preference-tuning-with-dpo/instruction-data-with-preference.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d2b9a-d6d2-46e2-89f8-5ab87e040e3b",
   "metadata": {
    "id": "725d2b9a-d6d2-46e2-89f8-5ab87e040e3b"
   },
   "source": [
    "- 두 개의 예시 항목을 살펴보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c11916f-9a26-4367-a16e-7b0c121a20a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c11916f-9a26-4367-a16e-7b0c121a20a6",
    "outputId": "00a432cc-19b1-484f-80e2-e897ee5e4024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Identify the correct spelling of the following word.',\n",
      " 'input': 'Ocassion',\n",
      " 'output': \"The correct spelling is 'Occasion.'\",\n",
      " 'rejected': \"The correct spelling is obviously 'Occasion.'\",\n",
      " 'chosen': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pp(data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ef804a-8c13-4a0b-9b2e-b65a4d0a870d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01ef804a-8c13-4a0b-9b2e-b65a4d0a870d",
    "outputId": "078cd643-83fb-4b42-ecf9-3256e8c9d239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': \"What is an antonym of 'complicated'?\",\n",
      " 'input': '',\n",
      " 'output': \"An antonym of 'complicated' is 'simple'.\",\n",
      " 'chosen': \"A suitable antonym for 'complicated' would be 'simple'.\",\n",
      " 'rejected': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db5697-a089-4b40-a1f3-e928e8018220",
   "metadata": {
    "id": "56db5697-a089-4b40-a1f3-e928e8018220"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "- 위에서 볼 수 있듯이 데이터셋은 5개의 키로 구성되어 있습니다:\n",
    "    - `'instruction'`과 `'input'`은 LLM 입력으로 사용됩니다.\n",
    "    - `'output'`은 7장에서 지시 튜닝 단계를 통해 훈련된 모델의 응답을 포함합니다.\n",
    "    - `'chosen'`과 `'rejected'` 항목은 DPO에 사용하는 항목입니다. 여기서 `'chosen'`은 선호되는 응답(preferred)이고, `'rejected'`는 거부되는 응답(dispreferred)입니다.\n",
    "- 목표는 모델이 거부된 응답 대신 선택된 응답의 스타일을 따르도록 하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86257468-a6ab-4ba3-9c9f-2fdc2c0cc284",
   "metadata": {
    "id": "86257468-a6ab-4ba3-9c9f-2fdc2c0cc284"
   },
   "source": [
    "- 아래는 7장([../01_main-chapter-code/ch07.ipynb](../01_main-chapter-code/ch07.ipynb))과 유사하게 Alpaca 프롬프트 스타일을 적용하여 모델 입력을 포맷팅하는 유틸리티 함수입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4564d55c-1c5d-46a6-b5e8-46ab568ad627",
   "metadata": {
    "id": "4564d55c-1c5d-46a6-b5e8-46ab568ad627"
   },
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f38b49f-63fd-48c5-bde8-a4717b7923ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f38b49f-63fd-48c5-bde8-a4717b7923ea",
    "outputId": "9ad07c59-05b3-42ae-c5bc-68780aaf6780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9e4c9-88a3-463a-8c16-c60ed7e6b51e",
   "metadata": {
    "id": "7dd9e4c9-88a3-463a-8c16-c60ed7e6b51e"
   },
   "source": [
    "- 마찬가지로 Alpaca 프롬프트 스타일을 사용하여 선호 응답과 거부 응답을 포맷팅할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad5831a-e936-44e5-a5cf-02953fe7d848",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ad5831a-e936-44e5-a5cf-02953fe7d848",
    "outputId": "2c0a0cbf-c13d-43cf-fcc1-a4585c21e66f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "desired_response = f\"### Response:\\n{data[50]['chosen']}\"\n",
    "print(desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc0991f6-fef7-48ab-8dee-fbd2863f784c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc0991f6-fef7-48ab-8dee-fbd2863f784c",
    "outputId": "cd85406c-3470-48f8-9792-63f91affd50a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "The correct spelling is obviously 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "possible_response = f\"### Response:\\n{data[50]['rejected']}\"\n",
    "print(possible_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6G3j2Q987t_g",
   "metadata": {
    "id": "6G3j2Q987t_g"
   },
   "source": [
    "&nbsp;\n",
    "## 2.2) 훈련, 검증 및 테스트 분할 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce2b1e-32d7-414c-8e6b-01f21a2488c2",
   "metadata": {
    "id": "53ce2b1e-32d7-414c-8e6b-01f21a2488c2"
   },
   "source": [
    "- 다음으로 데이터셋을 훈련 데이터 85%, 검증 데이터 5%, 테스트 데이터 10%의 세 부분으로 나눕니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36c7b919-8531-4e33-aebf-aaf8e6dbcfbd",
   "metadata": {
    "id": "36c7b919-8531-4e33-aebf-aaf8e6dbcfbd"
   },
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85%는 훈련용 (85% for training)\n",
    "test_portion = int(len(data) * 0.1)    # 10%는 테스트용 (10% for testing)\n",
    "val_portion = len(data) - train_portion - test_portion  # 나머지 5%는 검증용 (Remaining 5% for validation)\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "831a6c1b-119b-4622-9862-87f1db36e066",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "831a6c1b-119b-4622-9862-87f1db36e066",
    "outputId": "8e017483-1a75-4336-9540-ac6a69104e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d09f7-66af-49ed-8b9e-484f46e6a68d",
   "metadata": {
    "id": "c07d09f7-66af-49ed-8b9e-484f46e6a68d"
   },
   "source": [
    "&nbsp;\n",
    "## 2.3) `PreferenceDataset` 클래스와 배치 처리 함수 개발"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86101174-00c8-485d-8273-d086d5311926",
   "metadata": {
    "id": "86101174-00c8-485d-8273-d086d5311926"
   },
   "source": [
    "- 이 섹션에서는 7장([../01_main-chapter-code/ch07.ipynb](../01_main-chapter-code/ch07.ipynb))의 `InstructionDataset` 클래스를 DPO에 맞게 재작성합니다.\n",
    "- 즉, 단일 출력 시퀀스(응답)에 초점을 맞추는 대신, 하나가 다른 것보다 선호되는(\"chosen\") 한 쌍의 응답(다른 하나는 \"rejected\")을 반환하도록 데이터셋 클래스를 수정합니다.\n",
    "- 전반적으로 `PreferenceDataset`은 7장에서 사용된 `InstructionDataset`과 거의 동일합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08ad74-6dd4-4e40-b1e5-bc5f037d3d27",
   "metadata": {
    "id": "db08ad74-6dd4-4e40-b1e5-bc5f037d3d27"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PreferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    선호도(Preference) 학습을 위한 데이터셋 클래스입니다.\n",
    "    DPO나 Reward Model 학습 시 사용되며, \n",
    "    하나의 질문에 대해 '선택된 답변(Chosen)'과 '거부된 답변(Rejected)' 쌍을 다룹니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # 데이터를 미리 모두 토큰화해서 메모리에 올려둡니다 (속도 향상)\n",
    "        self.encoded_texts = []\n",
    "        \n",
    "        for entry in data:\n",
    "            # 1. 질문(Prompt) 포맷팅\n",
    "            prompt = format_input(entry)\n",
    "            \n",
    "            # 2. 데이터에서 승리한 답변(Chosen)과 패배한 답변(Rejected) 추출\n",
    "            rejected_response = entry[\"rejected\"]\n",
    "            chosen_response = entry[\"chosen\"]\n",
    "\n",
    "            # 3. 토큰화 진행 (세 가지를 준비합니다)\n",
    "            \n",
    "            # (1) 프롬프트(질문) 부분만 별도로 토큰화 \n",
    "            # -> 나중에 Loss 계산 시 질문 부분은 정답에서 제외(마스킹)하기 위해 길이 정보가 필요할 수 있습니다.\n",
    "            prompt_tokens = tokenizer.encode(prompt)\n",
    "            \n",
    "            # (2) [질문 + 좋은 답변] 전체 텍스트 생성 및 토큰화\n",
    "            chosen_full_text = f\"{prompt}\\n\\n### Response:\\n{chosen_response}\"\n",
    "            chosen_full_tokens = tokenizer.encode(chosen_full_text)\n",
    "            \n",
    "            # (3) [질문 + 나쁜 답변] 전체 텍스트 생성 및 토큰화\n",
    "            rejected_full_text = f\"{prompt}\\n\\n### Response:\\n{rejected_response}\"\n",
    "            rejected_full_tokens = tokenizer.encode(rejected_full_text)\n",
    "\n",
    "            # 4. 딕셔너리 형태로 저장\n",
    "            # 나중에 collate_fn에서 꺼내 쓸 때 명확하게 구분하기 위함입니다.\n",
    "            self.encoded_texts.append({\n",
    "                \"prompt\": prompt_tokens,      # 질문 토큰들\n",
    "                \"chosen\": chosen_full_tokens, # 질문 + 좋은 답변 토큰들\n",
    "                \"rejected\": rejected_full_tokens, # 질문 + 나쁜 답변 토큰들\n",
    "            })\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        인덱스에 해당하는 데이터 딕셔너리를 반환합니다.\n",
    "        예: {'prompt': [...], 'chosen': [...], 'rejected': [...]}\n",
    "        \"\"\"\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"전체 데이터 개수 반환\"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325d183-75b9-400a-80ac-0b8d2f526561",
   "metadata": {
    "id": "2325d183-75b9-400a-80ac-0b8d2f526561"
   },
   "source": [
    "- 업데이트된 `PreferenceDataset` 클래스와 함께, 각 배치의 시퀀스를 동일한 길이로 패딩(pad)하여 배치 단위로 조립할 수 있게 해주는 업데이트된 배치 콜레이션(collation) 함수도 필요합니다.\n",
    "- 프로세스를 설명하기 위해 아래 코드에 주석을 추가했습니다. 하지만 아래의 예시 입력과 출력을 보면 어떻게 작동하는지 이해하기 가장 쉬울 것입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a43a6-7704-4bff-9bbc-a38632374f30",
   "metadata": {
    "id": "8d3a43a6-7704-4bff-9bbc-a38632374f30"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    allowed_max_length=None,\n",
    "    mask_prompt_tokens=True,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    DPO/RLHF 학습을 위한 배치 처리 함수입니다.\n",
    "    Chosen(선호)과 Rejected(비선호) 응답을 패딩하고,\n",
    "    Loss 계산 시 '질문(Prompt)'과 '패딩' 부분을 무시하도록 마스크를 생성합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 배치 데이터를 담을 딕셔너리 초기화\n",
    "    # 나중에 텐서로 변환하여 모델에 입력됩니다.\n",
    "    batch_data = {\n",
    "        \"prompt\": [],\n",
    "        \"chosen\": [],\n",
    "        \"rejected\": [],\n",
    "        \"rejected_mask\": [],\n",
    "        \"chosen_mask\": []\n",
    "    }\n",
    "\n",
    "    # 2. 공통 최대 길이(Global Max Length) 찾기\n",
    "    # Chosen과 Rejected 데이터 중 가장 긴 문장을 기준으로 패딩 길이를 정합니다.\n",
    "    # (+1은 EOS 토큰 등을 위한 여유분일 수 있습니다)\n",
    "    max_length_common = 0\n",
    "    if batch:\n",
    "        for key in [\"chosen\", \"rejected\"]:\n",
    "            # 배치 내의 모든 아이템을 순회하며 가장 긴 길이를 갱신\n",
    "            current_max = max(len(item[key]) + 1 for item in batch)\n",
    "            max_length_common = max(max_length_common, current_max)\n",
    "\n",
    "    # 3. 배치의 각 항목(샘플) 처리 루프\n",
    "    for item in batch:\n",
    "        # 프롬프트(질문)는 길이를 계산하거나 참조용으로 저장 (Loss 계산엔 직접 안 쓰임)\n",
    "        prompt = torch.tensor(item[\"prompt\"])\n",
    "        batch_data[\"prompt\"].append(prompt)\n",
    "\n",
    "        # Chosen과 Rejected 각각에 대해 패딩 및 마스킹 처리\n",
    "        for key in [\"chosen\", \"rejected\"]:\n",
    "            sequence = item[key]\n",
    "            \n",
    "            # (1) 패딩(Padding): 공통 최대 길이만큼 뒤쪽을 pad_token_id로 채움\n",
    "            padded = sequence + [pad_token_id] * (max_length_common - len(sequence))\n",
    "            \n",
    "            # (2) 마스크 초기화: 일단 모든 위치를 1(True, 학습 함)로 설정\n",
    "            mask = torch.ones(len(padded)).bool()\n",
    "\n",
    "            # (3) 패딩 마스킹: 실제 데이터가 끝난 뒤쪽(패딩 영역)은 0(False, 학습 안 함)으로 설정\n",
    "            mask[len(sequence):] = False\n",
    "\n",
    "            # (4) 프롬프트 마스킹 (DPO의 핵심!)\n",
    "            # 질문(Prompt) 부분은 모델이 이미 알고 있는 내용이므로, \n",
    "            # 모델이 생성해야 할 '답변' 부분만 학습하기 위해 앞부분을 가립니다.\n",
    "            if mask_prompt_tokens:\n",
    "                # prompt.shape[0] : 질문의 길이\n",
    "                # +2 : \"\\n\\n\" 같은 구분자나 포맷팅용 특수 문자까지 포함해서 가리기 위함\n",
    "                mask[:prompt.shape[0] + 2] = False\n",
    "\n",
    "            # 처리된 데이터와 마스크를 리스트에 추가\n",
    "            batch_data[key].append(torch.tensor(padded))\n",
    "            batch_data[f\"{key}_mask\"].append(mask)\n",
    "\n",
    "    # 4. 텐서 변환 및 후처리\n",
    "    for key in [\"chosen\", \"rejected\", \"chosen_mask\", \"rejected_mask\"]:\n",
    "        # 리스트에 담긴 텐서들을 하나로 쌓음 (Batch화)\n",
    "        tensor_stack = torch.stack(batch_data[key])\n",
    "\n",
    "        # (선택 사항) 만약 설정된 최대 길이(allowed_max_length)가 있다면 자름(Truncate)\n",
    "        # 예: GPU 메모리 부족 방지\n",
    "        if allowed_max_length is not None:\n",
    "            tensor_stack = tensor_stack[:, :allowed_max_length]\n",
    "\n",
    "        # 최종 텐서를 지정된 장치(GPU/CPU)로 이동\n",
    "        batch_data[key] = tensor_stack.to(device)\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3744b-9bb0-4f1e-b66b-cff35ad8fd9f",
   "metadata": {
    "id": "76f3744b-9bb0-4f1e-b66b-cff35ad8fd9f"
   },
   "source": [
    "- 커스텀 콜레이트(collate) 함수를 사용하기 전에, 함수의 인수 중 일부를 미리 채운 버전을 만들어 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc137c-7ed7-4758-a518-cc4071b2817a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3cc137c-7ed7-4758-a518-cc4071b2817a",
    "outputId": "598e9def-9768-441a-f886-01f6ba6e250b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "# ============================================================\n",
    "# 1. 학습 장치(Device) 자동 설정\n",
    "# ============================================================\n",
    "\n",
    "# NVIDIA GPU(CUDA)가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# GPU가 없으면 기본 CPU 사용\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Collate 함수 설정 고정 (Partial Application)\n",
    "# ============================================================\n",
    "\n",
    "# 'partial'은 함수의 특정 인자값들을 미리 고정시켜 새로운 함수를 만드는 도구입니다.\n",
    "# DataLoader는 'batch'라는 인자 하나만 받는 함수를 원하기 때문에,\n",
    "# 나머지 복잡한 설정(device, length 등)을 미리 채워넣습니다.\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,          # 원래 정의했던 함수\n",
    "    device=device,              # 위에서 결정된 장치 (데이터를 자동으로 GPU로 올림)\n",
    "    mask_prompt_tokens=True,    # 프롬프트 마스킹 기능 활성화 (DPO/RLHF용)\n",
    "    allowed_max_length=1024     # 데이터가 너무 길면 1024 토큰에서 자름 (OOM 방지)\n",
    ")\n",
    "\n",
    "# 이제 'customized_collate_fn'은 'batch' 데이터만 넣으면 동작하는 함수가 되었습니다.\n",
    "# DataLoader(..., collate_fn=customized_collate_fn) 형태로 바로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29e996-e267-4348-bc1d-4ac6b725cf6a",
   "metadata": {
    "id": "5d29e996-e267-4348-bc1d-4ac6b725cf6a"
   },
   "source": [
    "- 이제 `customized_collate_fn`이 작동하는 모습을 확인하기 위해 선호도 데이터셋의 샘플 데이터에 적용해 보겠습니다. 이를 위해 처음 두 개의 항목을 가져옵니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1171057d-2a0f-48ff-bad6-4917a072f0f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1171057d-2a0f-48ff-bad6-4917a072f0f5",
    "outputId": "3db3eee8-db29-4ff6-8078-6577a05d953a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'instruction': 'Evaluate the following phrase by transforming it into the '\n",
      "                'spelling given.',\n",
      " 'input': 'freind --> friend',\n",
      " 'output': 'The spelling of the given phrase \"freind\" is incorrect, the '\n",
      "           'correct spelling is \"friend\".',\n",
      " 'rejected': 'The spelling of the given phrase \"freind\" is flat out wrong, get '\n",
      "             'it together, the correct spelling is \"friend\".',\n",
      " 'chosen': 'The spelling of the given phrase \"freind\" is incorrect, the '\n",
      "           'correct spelling is \"friend\".'}\n",
      "\n",
      "{'instruction': 'Edit the following sentence for grammar.',\n",
      " 'input': 'He go to the park every day.',\n",
      " 'output': 'He goes to the park every day.',\n",
      " 'rejected': 'He goes to the stupid park every single day.',\n",
      " 'chosen': 'He goes to the park every day.'}\n"
     ]
    }
   ],
   "source": [
    "example_data = data[:2]\n",
    "\n",
    "for i in example_data:\n",
    "    print()\n",
    "    pprint.pp(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1436cc-fbe5-4581-89d8-1992b5f04042",
   "metadata": {
    "id": "8f1436cc-fbe5-4581-89d8-1992b5f04042"
   },
   "source": [
    "- 다음으로, `example_dataset`을 인스턴스화하고 PyTorch `DataLoader`를 사용하여 나중에 모델 훈련에 사용할 데이터 로더를 모방한 `example_dataloader`를 생성합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db327575-c34b-4fea-b3c7-e30569c9be78",
   "metadata": {
    "id": "db327575-c34b-4fea-b3c7-e30569c9be78"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "example_dataset = PreferenceDataset(example_data, tokenizer)\n",
    "\n",
    "example_dataloader = DataLoader(\n",
    "    example_dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a446b7-7037-4d9a-9f14-b4ee0f6f37af",
   "metadata": {
    "id": "43a446b7-7037-4d9a-9f14-b4ee0f6f37af"
   },
   "source": [
    "- 데이터셋에는 다음과 같은 키(key)들이 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87ed4cf9-d70a-4bc7-b676-67e76ed3ee10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87ed4cf9-d70a-4bc7-b676-67e76ed3ee10",
    "outputId": "fa724d65-b0e1-4239-8090-9263135ad199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.keys: dict_keys(['prompt', 'chosen', 'rejected', 'rejected_mask', 'chosen_mask'])\n"
     ]
    }
   ],
   "source": [
    "for batch in example_dataloader:\n",
    "    break\n",
    "\n",
    "print(\"batch.keys:\", batch.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda3193-8c68-478c-98d8-0d9d880e7077",
   "metadata": {
    "id": "5bda3193-8c68-478c-98d8-0d9d880e7077"
   },
   "source": [
    "- 프롬프트(prompt)는 텐서 리스트이며, 각 텐서에는 해당 예시에 대한 토큰 ID가 포함되어 있습니다. 배치 크기를 2로 선택했으므로 여기에는 두 개의 토큰 ID 텐서 리스트가 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "468995ce-2906-498f-ac99-0a3f80d13d12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "468995ce-2906-498f-ac99-0a3f80d13d12",
    "outputId": "7f3df961-fcb5-4e49-9b0c-c99447c67cc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "           257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
       "         21017, 46486,    25,   198,    36,  2100,  4985,   262,  1708,  9546,\n",
       "           416, 25449,   340,   656,   262, 24993,  1813,    13,   198,   198,\n",
       "         21017, 23412,    25,   198, 19503,   521, 14610,  1545]),\n",
       " tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "           257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
       "         21017, 46486,    25,   198, 18378,   262,  1708,  6827,   329, 23491,\n",
       "            13,   198,   198, 21017, 23412,    25,   198,  1544,   467,   284,\n",
       "           262,  3952,   790,  1110,    13])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"prompt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cadebe-2516-4ae0-a71f-a8a623f2e1da",
   "metadata": {
    "id": "89cadebe-2516-4ae0-a71f-a8a623f2e1da"
   },
   "source": [
    "- 훈련 시 응답(response) 자체가 반드시 필요한 것은 아닙니다. 훈련 중 모델에 제공해야 할 것은 `\"chosen\"` 및 `\"rejected\"` 항목입니다.\n",
    "- `\"chosen\"` 및 `\"rejected\"` 응답 항목은 텐서로 쌓을 수 있도록 패딩(pad) 처리됩니다. 프롬프트와 마찬가지로 이러한 응답 텍스트는 토큰 ID로 인코딩됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8f49c56-3989-4fe9-81ac-6bb3cce1a5b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8f49c56-3989-4fe9-81ac-6bb3cce1a5b8",
    "outputId": "ccc0bd06-6e85-4ee9-893b-d985f26a835d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "           257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
       "         21017, 46486,    25,   198,    36,  2100,  4985,   262,  1708,  9546,\n",
       "           416, 25449,   340,   656,   262, 24993,  1813,    13,   198,   198,\n",
       "         21017, 23412,    25,   198, 19503,   521, 14610,  1545,   198,   198,\n",
       "         21017, 18261,    25,   198,   464, 24993,   286,   262,  1813,  9546,\n",
       "           366, 19503,   521,     1,   318, 11491,    11,   262,  3376, 24993,\n",
       "           318,   366,  6726,  1911, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256],\n",
       "        [21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "           257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
       "         21017, 46486,    25,   198, 18378,   262,  1708,  6827,   329, 23491,\n",
       "            13,   198,   198, 21017, 23412,    25,   198,  1544,   467,   284,\n",
       "           262,  3952,   790,  1110,    13,   198,   198, 21017, 18261,    25,\n",
       "           198,  1544,  2925,   284,   262,  3952,   790,  1110,    13, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"chosen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a4cd6d-b2ad-45a6-b00a-ba5b720be4ea",
   "metadata": {
    "id": "35a4cd6d-b2ad-45a6-b00a-ba5b720be4ea"
   },
   "source": [
    "- 위의 토큰 ID는 모델 입력을 나타내지만, 이 형식으로는 사람이 해석하기 어렵습니다.\n",
    "- 따라서 이를 다시 텍스트로 변환하여 더 쉽게 검사하고 해석할 수 있도록 작은 유틸리티 함수를 구현해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52ea54ba-32cb-4ecb-b38b-923f42fd4615",
   "metadata": {
    "id": "52ea54ba-32cb-4ecb-b38b-923f42fd4615"
   },
   "outputs": [],
   "source": [
    "def decode_tokens_from_batch(token_ids, tokenizer):\n",
    "    ids_in_python_list = token_ids.flatten().tolist()\n",
    "    return tokenizer.decode(ids_in_python_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9dd0ce-1fd4-419c-833f-ea5a1f8d800d",
   "metadata": {
    "id": "bc9dd0ce-1fd4-419c-833f-ea5a1f8d800d"
   },
   "source": [
    "- 배치의 첫 번째 프롬프트 항목에 `decode_tokens_from_batch` 유틸리티 함수를 적용해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55ee481e-3e2c-4ff6-b614-8cb18eb16a41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55ee481e-3e2c-4ff6-b614-8cb18eb16a41",
    "outputId": "17ddec15-a09d-45b5-b1e8-600cd59a9600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n"
     ]
    }
   ],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch[\"prompt\"][0],  # 배치의 첫 번째 항목 [0] (0 for the first entry in the batch)\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b95c4-d5c2-4492-9d19-a45b090eee7e",
   "metadata": {
    "id": "637b95c4-d5c2-4492-9d19-a45b090eee7e"
   },
   "source": [
    "- 위에서 볼 수 있듯이 프롬프트 형식이 올바르게 지정되었습니다. 이제 `\"chosen\"` 응답에 대해서도 동일한 작업을 수행해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33a24f20-5ec3-4a89-b57a-52e997163d07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33a24f20-5ec3-4a89-b57a-52e997163d07",
    "outputId": "e04366ee-3719-4b07-fcef-6e9dddc06310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n",
      "\n",
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch[\"chosen\"][0],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9fbdbd-1cff-401f-8e6c-cd98c134c0f2",
   "metadata": {
    "id": "ac9fbdbd-1cff-401f-8e6c-cd98c134c0f2"
   },
   "source": [
    "- 위에서 볼 수 있듯이 지시 튜닝과 유사하게, 훈련 중 모델에 전달되는 응답에는 입력 프롬프트도 포함됩니다.\n",
    "- 또한 패딩 토큰으로 `<|endoftext|>` 토큰을 포함시켰는데, 이는 응답을 배치로 쌓기 위해 비슷한 길이로 확장하는 데 필요합니다.\n",
    "- 걱정하지 마세요. `<|endoftext|>` 토큰은 나중에 손실 계산 시 무시되므로 훈련 결과에 영향을 미치지 않습니다.\n",
    "- 이제 해당하는 거부된(rejected) 응답도 살펴보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db382be5-c727-4299-8597-c05424ba9308",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db382be5-c727-4299-8597-c05424ba9308",
    "outputId": "edbd8c4a-0528-4361-aeba-9b3c3bbde33b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input:\n",
      "freind --> friend\n",
      "\n",
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is flat out wrong, get it together, the correct spelling is \"friend\".<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch[\"rejected\"][0],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715dc968-aa64-4388-b577-7c295831bdcf",
   "metadata": {
    "id": "715dc968-aa64-4388-b577-7c295831bdcf"
   },
   "source": [
    "- 이 경우 위에서 볼 수 있듯이, 거부된 응답은 선택된 응답의 더 무례한 버전입니다(우리는 모델이 무례한 응답을 생성하는 것을 원하지 않습니다).\n",
    "- 마지막으로 데이터 마스크에 대해 이야기해 보겠습니다. 위에서 구현한 커스텀 콜레이트 함수를 자세히 살펴보면 각 데이터셋 항목에 대해 `\"chosen_mask\"`와 `\"rejected_mask\"`를 생성했습니다.\n",
    "- 마스크는 아래의 `\"chosen\"` 항목에 대해 표시된 것처럼 응답 항목과 동일한 모양을 가집니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c324eab-cf1d-4071-b3ba-797d8ec4d1da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c324eab-cf1d-4071-b3ba-797d8ec4d1da",
    "outputId": "742a5742-1bc0-4f74-9eb9-cbf81f936ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen inputs: torch.Size([81])\n",
      "chosen mask:   torch.Size([81])\n"
     ]
    }
   ],
   "source": [
    "print(\"chosen inputs:\", batch[\"chosen\"][0].shape)\n",
    "print(\"chosen mask:  \", batch[\"chosen_mask\"][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e95f7-cfc3-4f5f-be5e-c279fba5f674",
   "metadata": {
    "id": "880e95f7-cfc3-4f5f-be5e-c279fba5f674"
   },
   "source": [
    "- 이 마스크의 내용은 불리언(`True` 및 `False`) 값입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da75b550-5da4-4292-9a7e-a05b842bdcb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da75b550-5da4-4292-9a7e-a05b842bdcb7",
    "outputId": "e5f012c3-33ba-4e6b-aa55-3e331865218f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False, False, False, False, False, False,\n",
       "        False], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"chosen_mask\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e67b862-4430-4c99-9157-90955dde29b6",
   "metadata": {
    "id": "0e67b862-4430-4c99-9157-90955dde29b6"
   },
   "source": [
    "- `True` 값은 실제 응답에 해당하는 토큰 ID를 나타냅니다.\n",
    "- `False` 토큰은 프롬프트 토큰(이전에 `customized_collate_fn` 함수에서 `mask_prompt_tokens=True`로 설정한 경우) 또는 패딩 토큰에 해당하는 토큰 ID입니다.\n",
    "- 따라서 마스크를 선택 마스크로 사용하여 아래와 같이 프롬프트 및 패딩 토큰을 모두 제거하고 응답에 해당하는 토큰 ID만 선택할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1114c6fe-524b-401c-b9fe-02260e6f0541",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1114c6fe-524b-401c-b9fe-02260e6f0541",
    "outputId": "6d99af1d-940a-4012-c5d9-21d463a66e40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n"
     ]
    }
   ],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch[\"chosen\"][0][batch[\"chosen_mask\"][0]],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a89f83a4-d16e-40d2-ba43-bd410affd967",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a89f83a4-d16e-40d2-ba43-bd410affd967",
    "outputId": "1d439c7e-c079-4594-d02a-fa83a3cb275d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is flat out wrong, get it together, the correct spelling is \"friend\".\n"
     ]
    }
   ],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch[\"rejected\"][0][batch[\"rejected_mask\"][0]],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525287f-137c-4d71-94ae-cfd6db7b057c",
   "metadata": {
    "id": "e525287f-137c-4d71-94ae-cfd6db7b057c"
   },
   "source": [
    "- 나중에 DPO 손실을 계산할 때 이 마스크를 사용하여 프롬프트 및 패딩 토큰을 무시할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jbafhM_R8z5q",
   "metadata": {
    "id": "jbafhM_R8z5q"
   },
   "source": [
    "&nbsp;\n",
    "## 2.4) 훈련, 검증 및 테스트 데이터 로더 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c29eb8-d1b9-4abe-a155-52b3270d759a",
   "metadata": {
    "id": "b3c29eb8-d1b9-4abe-a155-52b3270d759a"
   },
   "source": [
    "- 위에서 설명을 위해 선호도 데이터셋의 작은 예제 부분집합으로 작업했습니다.\n",
    "- 이제 실제 훈련, 검증 및 테스트 데이터 로더를 생성해 보겠습니다.\n",
    "- 이 과정은 사전 훈련 및 지시 튜닝 챕터에서 데이터 로더를 생성하는 과정과 동일하므로 별도의 설명은 생략합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c0068bf-bda0-4d9e-9f79-2fc4b94cbd1c",
   "metadata": {
    "id": "5c0068bf-bda0-4d9e-9f79-2fc4b94cbd1c"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = PreferenceDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f4a257b-6835-4194-abe2-5831d6a44885",
   "metadata": {
    "id": "2f4a257b-6835-4194-abe2-5831d6a44885"
   },
   "outputs": [],
   "source": [
    "val_dataset = PreferenceDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = PreferenceDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1ba19-a6d5-4a77-8283-7a17d7ec06e2",
   "metadata": {
    "id": "1fe1ba19-a6d5-4a77-8283-7a17d7ec06e2"
   },
   "source": [
    "- 데이터 로더를 반복하며 데이터셋 모양을 살펴보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80d61f15-facb-4eb8-a9be-6427887d24b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80d61f15-facb-4eb8-a9be-6427887d24b2",
    "outputId": "dacd3bdf-f069-4b36-da2c-d6c1c6cc5405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 94]) torch.Size([8, 94])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 99]) torch.Size([8, 99])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 97]) torch.Size([8, 97])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 101]) torch.Size([8, 101])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 100]) torch.Size([8, 100])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 93]) torch.Size([8, 93])\n",
      "torch.Size([8, 115]) torch.Size([8, 115])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 95]) torch.Size([8, 95])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 94]) torch.Size([8, 94])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 90]) torch.Size([8, 90])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 93]) torch.Size([8, 93])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 99]) torch.Size([8, 99])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 86]) torch.Size([8, 86])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 97]) torch.Size([8, 97])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 93]) torch.Size([8, 93])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 93]) torch.Size([8, 93])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 90]) torch.Size([8, 90])\n",
      "torch.Size([8, 99]) torch.Size([8, 99])\n",
      "torch.Size([8, 104]) torch.Size([8, 104])\n",
      "torch.Size([8, 101]) torch.Size([8, 101])\n",
      "torch.Size([8, 98]) torch.Size([8, 98])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 85]) torch.Size([8, 85])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for batch in train_loader:\n",
    "    print(\n",
    "        batch[\"chosen\"].shape,\n",
    "        batch[\"rejected\"].shape,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff958a6-5e61-49f5-9a97-360aa34e3758",
   "metadata": {
    "id": "7ff958a6-5e61-49f5-9a97-360aa34e3758"
   },
   "source": [
    "- 각 행은 각 배치에 있는 `\"chosen\"` 및 `\"rejected\"` 항목의 모양(shape)을 보여줍니다.\n",
    "- 배치 단위로 패딩을 적용했기 때문에 각 행의 모양이 다릅니다.\n",
    "- 이는 전체 데이터셋에서 가장 긴 샘플에 맞춰 모든 샘플을 패딩하는 것은 비효율적이기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb0543-1142-4374-8825-3384e20c6ac0",
   "metadata": {
    "id": "29cb0543-1142-4374-8825-3384e20c6ac0"
   },
   "source": [
    "&nbsp;\n",
    "# 3) DPO 정렬을 위한 미세 튜닝된 LLM 로드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b08881-b769-4b26-8153-5ec0e8573ed2",
   "metadata": {
    "id": "22b08881-b769-4b26-8153-5ec0e8573ed2"
   },
   "source": [
    "- RLHF나 DPO와 같은 LLM 정렬 단계는 이미 지시 튜닝된 모델이 있다고 가정합니다.\n",
    "- 이 섹션에는 7장에서 지시 튜닝되어 저장된 모델을 로드하는 최소한의 코드가 포함되어 있습니다 ([../01_main-chapter-code/ch07.ipynb](../01_main-chapter-code/ch07.ipynb)).\n",
    "- 진행하기 전에 먼저 7장 코드를 실행하여 지시 튜닝된 모델을 생성했는지 확인하세요.\n",
    "- 아래 코드는 지시 튜닝된 모델을 현재 디렉토리로 복사합니다:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8585e-4569-4033-84a7-3903d0e8aaf8",
   "metadata": {
    "id": "71c8585e-4569-4033-84a7-3903d0e8aaf8"
   },
   "source": [
    "- 다음으로, 이전 챕터의 기본 구성을 재사용하여 모델 가중치를 로드합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8333fee-e7fe-4f8c-9411-8c1db6252d98",
   "metadata": {
    "id": "a8333fee-e7fe-4f8c-9411-8c1db6252d98"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import GPTModel\n",
    "# `previous_chapters.py` 파일이 로컬에 없는 경우,\n",
    "# `llms-from-scratch` PyPI 패키지에서 가져올 수 있습니다.\n",
    "# 자세한 내용은 다음을 참조하세요: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# 예:\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 어휘 사전 크기 (Vocabulary size)\n",
    "    \"context_length\": 1024,  # 컨텍스트 길이 (Context length)\n",
    "    \"drop_rate\": 0.0,        # 드롭아웃 비율 (Dropout rate)\n",
    "    \"qkv_bias\": True         # Query-key-value 편향 (Query-key-value bias)\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2821403-605c-4071-a4ff-e23f4c9a11fd",
   "metadata": {
    "id": "c2821403-605c-4071-a4ff-e23f4c9a11fd"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"outputs/gpt2-medium355M-sft.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "        weights_only=True\n",
    "    )\n",
    ")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61863bec-bd42-4194-b994-645bfe2df8be",
   "metadata": {
    "id": "61863bec-bd42-4194-b994-645bfe2df8be"
   },
   "source": [
    "- DPO로 로드된 모델을 훈련하기 전에, 샘플 데이터로 테스트하여 미세 튜닝된 모델이 올바르게 저장되고 로드되었는지 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4357aec5-0db2-4d73-b37b-539cd8fa80a3",
   "metadata": {
    "id": "4357aec5-0db2-4d73-b37b-539cd8fa80a3"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response\n",
    "that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "541e7988-38d3-47f6-bd52-9da6564479fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "541e7988-38d3-47f6-bd52-9da6564479fa",
    "outputId": "278f7ddf-37c2-4c3a-d069-c510ef6f8d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response\n",
      "that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Response:\n",
      "The meal is cooked every day by the chef.\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "# 또는: (Alternatively:)\n",
    "# from llms_from_scratch.ch05 (\n",
    "#     generate,\n",
    "#     text_to_token_ids,\n",
    "#     token_ids_to_text\n",
    "# )\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(prompt, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n",
    "\n",
    "response = token_ids_to_text(token_ids, tokenizer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87ed19-fded-4e56-8585-6c7c0367b354",
   "metadata": {
    "id": "be87ed19-fded-4e56-8585-6c7c0367b354"
   },
   "source": [
    "- 위에서 볼 수 있듯이 모델은 합리적이고 정확한 응답을 제공합니다.\n",
    "- 7장에서 설명한 것처럼, 실제로는 응답을 정리하여 프롬프트와 프롬프트 스타일이 제거된 응답 텍스트만 반환하도록 합니다(예: ChatGPT에서 익숙한 방식과 유사하게):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c30c4e2-af84-4ab4-95d0-9641e32c1e7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c30c4e2-af84-4ab4-95d0-9641e32c1e7f",
    "outputId": "70192bbe-fdf6-43eb-c673-f573f8c70156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meal is cooked every day by the chef.\n"
     ]
    }
   ],
   "source": [
    "def extract_response(response_text, input_text):\n",
    "    return response_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "response = extract_response(response, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80442cb9-83b1-46b8-bad0-7d44297ca52d",
   "metadata": {
    "id": "80442cb9-83b1-46b8-bad0-7d44297ca52d"
   },
   "source": [
    "- 이제 DPO 부분으로 넘어갈 준비가 거의 되었습니다.\n",
    "- 이 노트북의 시작 부분에서 언급했듯이, DPO는 두 개의 LLM, 즉 정책 모델(policy model, 최적화하려는 LLM)과 참조 모델(reference model, 변경하지 않고 유지하는 원본 모델)을 사용합니다.\n",
    "- 아래에서 `model`의 이름을 `policy_model`로 변경하고, `reference_model`이라고 하는 모델의 두 번째 인스턴스를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88cc3a-312e-4b29-bc6d-de8354c1eb9f",
   "metadata": {
    "id": "5d88cc3a-312e-4b29-bc6d-de8354c1eb9f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. 학습할 모델 (Policy Model) 설정\n",
    "# 'model'은 앞선 코드에서 이미 로드되었거나 생성된 객체입니다.\n",
    "# 이 모델은 학습이 진행됨에 따라 가중치(Weight)가 계속 업데이트됩니다.\n",
    "policy_model = model\n",
    "\n",
    "# 2. 기준 모델 (Reference Model) 생성\n",
    "# 학습 중에 변하지 않고 고정된 기준점 역할을 할 모델을 새로 만듭니다.\n",
    "# 보통 SFT(Supervised Fine-Tuning)가 완료된 시점의 가중치를 그대로 사용합니다.\n",
    "reference_model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "# 3. 기준 모델에 가중치 로드\n",
    "reference_model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"outputs/gpt2-medium355M-sft.pth\",  # 미리 학습된 SFT 모델 체크포인트 경로\n",
    "        map_location=torch.device(\"cpu\"),   # 메모리 관리를 위해 일단 CPU로 로드\n",
    "        weights_only=True                   # 보안 설정: 피클링된 데이터 중 가중치만 로드 (안전)\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4. 기준 모델을 평가 모드(Evaluation Mode)로 전환\n",
    "# [매우 중요] Reference Model은 학습되지 않아야 하므로 Dropout 등이 동작하지 않게 설정합니다.\n",
    "# (보통 뒤에서 requires_grad = False 설정도 추가로 해주는 것이 좋습니다.)\n",
    "reference_model.eval()\n",
    "\n",
    "# 5. 두 모델을 연산 장치(GPU/MPS 등)로 이동\n",
    "policy_model.to(device)\n",
    "reference_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c1469-0038-4914-8aa5-15b1f81877cc",
   "metadata": {
    "id": "9c6c1469-0038-4914-8aa5-15b1f81877cc"
   },
   "source": [
    "&nbsp;\n",
    "# 4) DPO 손실 함수 코딩하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbe60c-e4ce-413e-beec-22eff0237d11",
   "metadata": {
    "id": "75dbe60c-e4ce-413e-beec-22eff0237d11"
   },
   "source": [
    "- 이전 섹션에서 모델 로딩과 데이터셋 준비를 마쳤으므로, 이제 재미있는 부분인 DPO 손실 코딩으로 넘어갈 수 있습니다.\n",
    "- 아래의 DPO 손실 코드는 [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290) 논문에서 제안한 방법을 기반으로 합니다.\n",
    "- 참고로 핵심 DPO 수식은 다음과 같습니다:\n",
    "\n",
    "<img src=\"images/llm_from_scratch/dpo/3.webp\" width=800px>\n",
    "\n",
    "- 위 식에서,\n",
    "  - \"기댓값(expected value)\" $\\mathbb{E}$는 통계 용어로 확률 변수(괄호 안의 표현식)의 평균값을 의미합니다. $-\\mathbb{E}$를 최적화하면 모델이 사용자 선호도에 더 잘 부합하게 됩니다.\n",
    "  - $\\pi_{\\theta}$ 변수는 소위 정책(policy, 강화 학습에서 차용한 용어)이라 불리며 우리가 최적화하고자 하는 LLM을 나타냅니다. $\\pi_{ref}$는 참조(reference) LLM으로, 일반적으로 최적화 전의 원본 LLM입니다 (훈련 시작 시점에는 $\\pi_{\\theta}$와 $\\pi_{ref}$가 보통 동일합니다).\n",
    "  - $\\beta$는 $\\pi_{\\theta}$와 참조 모델 간의 발산(divergence)을 제어하는 하이퍼파라미터입니다. $\\beta$를 높이면 전체 손실 함수에서 $\\pi_{\\theta}$와 $\\pi_{ref}$의 로그 확률 차이가 미치는 영향이 커져서, 두 모델 간의 차이(divergence)가 증가하게 됩니다.\n",
    "  - 로지스틱 시그모이드 함수 $\\sigma(\\centerdot)$는 선호되는 응답과 거부되는 응답의 로그 오즈(log-odds, 시그모이드 함수 내부의 항)를 확률 점수로 변환합니다.\n",
    "- 코드로 DPO 손실을 다음과 같이 구현할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38CsrrwJIZiV",
   "metadata": {
    "id": "38CsrrwJIZiV"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_dpo_loss(\n",
    "    model_chosen_logprobs,\n",
    "    model_rejected_logprobs,\n",
    "    reference_chosen_logprobs,\n",
    "    reference_rejected_logprobs,\n",
    "    beta=0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    DPO(Direct Preference Optimization) 손실 함수를 계산합니다.\n",
    "    학습 중인 모델(Policy)이 고정된 기준 모델(Reference)보다\n",
    "    '선호되는 답변(Chosen)'에 더 높은 확률을 부여하도록 유도합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. [학습 모델]의 선호도 격차 계산 (Model Log Ratios)\n",
    "    # 수식: log(Chosen확률) - log(Rejected확률) = log(Chosen확률 / Rejected확률)\n",
    "    # 의미: 학습 모델이 Rejected보다 Chosen을 얼마나 더 선호하는가?\n",
    "    # 값이 클수록 정답을 더 강하게 확신한다는 뜻입니다.\n",
    "    model_logratios = model_chosen_logprobs - model_rejected_logprobs\n",
    "\n",
    "    # 2. [참조 모델]의 선호도 격차 계산 (Reference Log Ratios)\n",
    "    # 의미: 원래 모델(기준점)은 Rejected보다 Chosen을 얼마나 더 선호했는가?\n",
    "    # 이 값은 학습 중에 변하지 않는 기준선(Base line) 역할을 합니다.\n",
    "    reference_logratios = reference_chosen_logprobs - reference_rejected_logprobs\n",
    "\n",
    "    # 3. 두 모델의 격차 비교 (Logits)\n",
    "    # 수식: (학습 모델의 확신도) - (참조 모델의 확신도)\n",
    "    # 목적: 참조 모델이 확신하는 정도보다 '더' 정답을 확신하게 만들어야 합니다.\n",
    "    logits = model_logratios - reference_logratios\n",
    "\n",
    "    # 4. 최종 Loss 계산 (DPO 공식의 핵심)\n",
    "    # 공식: -log(sigmoid(beta * logits))\n",
    "    # 원리:\n",
    "    #   - logits가 양수(학습 모델이 참조 모델보다 더 잘함) -> sigmoid는 1에 가깝고 -> -log(1)은 0 (Loss 낮음)\n",
    "    #   - logits가 음수(학습 모델이 못함) -> sigmoid는 0에 가깝고 -> -log(0)은 무한대 (Loss 높음)\n",
    "    # beta: 학습 강도를 조절하는 하이퍼파라미터 (KL Divergence 제약의 역수 역할)\n",
    "    losses = -F.logsigmoid(beta * logits)\n",
    "\n",
    "    # 5. [모니터링용] 암시적 보상 (Implicit Rewards) 계산\n",
    "    # DPO는 별도의 리워드 모델이 없지만, 수식적으로 리워드를 역산해볼 수 있습니다.\n",
    "    # 보상 = log(학습모델 확률 / 참조모델 확률) -> 학습 모델이 참조 모델보다 확률을 얼마나 높였는지 측정\n",
    "    # detach(): 이 계산은 학습(역전파)에는 쓰이지 않고 기록용으로만 쓰므로 그래디언트를 끊습니다.\n",
    "    chosen_rewards = (model_chosen_logprobs - reference_chosen_logprobs).detach()\n",
    "    rejected_rewards = (model_rejected_logprobs - reference_rejected_logprobs).detach()\n",
    "\n",
    "    # 배치 내 모든 샘플의 평균을 반환\n",
    "    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693be65b-38fc-4d18-bf53-a260a15436e1",
   "metadata": {
    "id": "693be65b-38fc-4d18-bf53-a260a15436e1"
   },
   "source": [
    "- 로그에 익숙하시다면, 위 코드에서 적용된 일반적인 관계식 $\\log\\left(\\frac{a}{b}\\right) = \\log a - \\log b$를 참고하세요.\n",
    "- 이 점을 염두에 두고 몇 가지 단계를 살펴보겠습니다 (나중에 별도의 함수를 사용하여 `logprobs`를 계산할 것입니다).\n",
    "- 다음 라인부터 시작해 봅시다.\n",
    "\n",
    "    ```python\n",
    "    model_logratios = model_chosen_logprobs - model_rejected_logprobs\n",
    "    reference_logratios = reference_chosen_logprobs - reference_rejected_logprobs\n",
    "    ```\n",
    "\n",
    "- 위 라인들은 정책 모델과 참조 모델 모두에 대해 선택된(chosen) 샘플과 거부된(rejected) 샘플 간의 로그 확률(로짓) 차이를 계산합니다 (이는 $\\log\\left(\\frac{a}{b}\\right) = \\log a - \\log b$이기 때문입니다):\n",
    "\n",
    "$$\\log \\left( \\frac{\\pi_\\theta (y_w \\mid x)}{\\pi_\\theta (y_l \\mid x)} \\right) \\quad \\text{and} \\quad \\log \\left( \\frac{\\pi_{\\text{ref}}(y_w \\mid x)}{\\pi_{\\text{ref}}(y_l \\mid x)} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458d217-e0ad-40a5-925c-507a8fcf5795",
   "metadata": {
    "id": "5458d217-e0ad-40a5-925c-507a8fcf5795"
   },
   "source": [
    "- 다음으로, `logits = model_logratios - reference_logratios` 코드는 모델의 로그 비율과 참조 모델의 로그 비율 간의 차이를 계산합니다. 즉,\n",
    "\n",
    "$$\\beta \\log \\left( \\frac{\\pi_\\theta (y_w \\mid x)}{\\pi_{\\text{ref}} (y_w \\mid x)} \\right)\n",
    "- \\beta \\log \\left( \\frac{\\pi_\\theta (y_l \\mid x)}{\\pi_{\\text{ref}} (y_l \\mid x)} \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e3e36-f5f1-407f-b662-4c20a0ac0354",
   "metadata": {
    "id": "f18e3e36-f5f1-407f-b662-4c20a0ac0354"
   },
   "source": [
    "- 마지막으로, `losses = -F.logsigmoid(beta * logits)`는 로그-시그모이드 함수를 사용하여 손실을 계산합니다. 원래 방정식에서 기대값 내부의 항은 다음과 같습니다.\n",
    "\n",
    "$$\\log \\sigma \\left( \\beta \\log \\left( \\frac{\\pi_\\theta (y_w \\mid x)}{\\pi_{\\text{ref}} (y_w \\mid x)} \\right)\n",
    "- \\beta \\log \\left( \\frac{\\pi_\\theta (y_l \\mid x)}{\\pi_{\\text{ref}} (y_l \\mid x)} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6f92d-7d64-41fe-bcaa-2bddd46027e1",
   "metadata": {
    "id": "00a6f92d-7d64-41fe-bcaa-2bddd46027e1"
   },
   "source": [
    "- 위에서는 로그 확률이 이미 계산되었다고 가정했습니다. 이제 위에서 `compute_dpo_loss` 함수에 전달된 로그 확률, 즉 $\\pi_\\theta (y_w \\mid x)$, ${\\pi_\\theta (y_l \\mid x)}$ 등의 값을 계산하는 데 사용할 `compute_logprobs` 함수를 정의해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6507b-d2e2-4469-86b9-f057b08b5df9",
   "metadata": {
    "id": "71e6507b-d2e2-4469-86b9-f057b08b5df9"
   },
   "outputs": [],
   "source": [
    "def compute_logprobs(logits, labels, selection_mask=None):\n",
    "    \"\"\"\n",
    "    모델의 출력(Logits)과 정답(Labels)을 비교하여,\n",
    "    모델이 정답 토큰들에 부여한 '로그 확률'의 평균을 계산합니다.\n",
    "\n",
    "    Args:\n",
    "        logits: 모델이 뱉어낸 예측값 (아직 확률로 변환 전). Shape: (배치크기, 문장길이, 단어장크기)\n",
    "        labels: 실제 정답 단어들의 ID. Shape: (배치크기, 문장길이)\n",
    "        selection_mask: 패딩이나 질문(Prompt) 등 계산에서 제외할 부분을 표시한 마스크.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. [핵심] 위치 정렬 (Next Token Prediction)\n",
    "    # LLM은 '현재 단어'를 보고 '다음 단어'를 맞추는 모델입니다.\n",
    "    # 따라서 입력(Logits)의 t번째 토큰은 정답(Labels)의 t+1번째 토큰을 예측해야 합니다.\n",
    "    \n",
    "    # labels[:, 1:]: 첫 번째 토큰은 예측 대상이 아니므로 제외 (오른쪽으로 한 칸 이동 효과)\n",
    "    labels = labels[:, 1:].clone()\n",
    "    \n",
    "    # logits[:, :-1, :]: 마지막 토큰에 대한 예측은 맞춰볼 정답(다음 토큰)이 없으므로 제외\n",
    "    logits = logits[:, :-1, :]\n",
    "\n",
    "    # 2. 확률 변환 (Log Softmax)\n",
    "    # 모델의 날것 점수(Logits)를 확률 분포(Log Probability)로 변환합니다.\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # 3. 정답에 해당하는 확률만 추출 (Gather)\n",
    "    # 단어장 전체(5만 개 등)에 대한 확률 중, 실제로 등장한 '정답 단어'의 확률만 콕 집어냅니다.\n",
    "    selected_log_probs = torch.gather(\n",
    "        input=log_probs,\n",
    "        dim=-1,\n",
    "        index=labels.unsqueeze(-1) # 정답 인덱스에 맞춰 차원 확장\n",
    "    ).squeeze(-1) # 다시 차원 축소 (배치크기, 문장길이-1)\n",
    "\n",
    "    # 4. 마스킹 및 평균 계산\n",
    "    if selection_mask is not None:\n",
    "        # 마스크도 위에서 잘라낸 labels와 길이를 맞추기 위해 앞부분을 자름\n",
    "        mask = selection_mask[:, 1:].clone()\n",
    "\n",
    "        # 마스크 적용:\n",
    "        # 패딩이나 프롬프트(질문) 부분의 로그 확률은 0으로 만들어버려서 합계에 영향이 없게 함\n",
    "        selected_log_probs = selected_log_probs * mask\n",
    "\n",
    "        # 평균 계산:\n",
    "        # (유효한 토큰들의 로그 확률 합) / (유효한 토큰의 개수)\n",
    "        # sum(-1)은 문장 길이 방향으로 합계를 구함 -> 결과 Shape: (batch_size,)\n",
    "        avg_log_prob = selected_log_probs.sum(-1) / mask.sum(-1)\n",
    "\n",
    "        return avg_log_prob\n",
    "\n",
    "    else:\n",
    "        # 마스크가 없으면 그냥 단순 평균\n",
    "        return selected_log_probs.mean(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a71ac-3fcc-44a4-befc-1c56bbd378d7",
   "metadata": {
    "id": "cf6a71ac-3fcc-44a4-befc-1c56bbd378d7"
   },
   "source": [
    "- 위의 함수는 `torch.gather` 함수 때문에 처음에는 조금 복잡해 보일 수 있지만, 내부적으로 PyTorch의 `cross_entropy` 함수에서 일어나는 과정과 매우 유사합니다.\n",
    "- 예를 들어, 다음 예시를 살펴봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59873470-464d-4be2-860f-cbb7ac2d80ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59873470-464d-4be2-860f-cbb7ac2d80ba",
    "outputId": "8f7b47d4-73fe-4605-c17d-ad6cfd909a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4185) tensor(1.4185)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================================================\n",
    "# 1. 데이터 준비\n",
    "# =========================================================\n",
    "\n",
    "# 모델의 예측값 (Logits): 아직 확률로 변환되기 전의 날것 점수\n",
    "# 상황: 데이터 2개(Batch=2), 분류할 클래스 3개(Class=3)\n",
    "logits = torch.tensor(\n",
    "    [[2.0, 1.0, 0.1],   # 첫 번째 샘플 예측값 (0번 클래스 점수가 제일 높음)\n",
    "     [0.5, 2.5, 0.3]])  # 두 번째 샘플 예측값 (1번 클래스 점수가 제일 높음)\n",
    "    # Shape: (2, 3)\n",
    "\n",
    "# 실제 정답 (Targets): 각 샘플의 정답 클래스 인덱스\n",
    "targets = torch.tensor([0, 2])  \n",
    "# 값의 의미: 첫 번째 샘플 정답 -> 0번 클래스, 두 번째 샘플 정답 -> 2번 클래스\n",
    "# Shape: (2,)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. 수동으로 손실(Loss) 계산해보기 (내부 원리)\n",
    "# 원리: CrossEntropy = LogSoftmax + NLL(Negative Log Likelihood)\n",
    "# =========================================================\n",
    "\n",
    "# (1) Log Softmax 적용\n",
    "# 점수(logits)를 확률(0~1)로 바꾸고(Softmax), 거기에 로그(Log)를 씌웁니다.\n",
    "log_softmax_logits = F.log_softmax(logits, dim=1)  # Shape: (2, 3)\n",
    "\n",
    "# (2) 정답 클래스의 확률만 쏙 뽑아내기 (Gather)\n",
    "# gather를 쓰기 위해 targets의 모양을 (2,) -> (2, 1)로 맞춰줍니다.\n",
    "# 예: [0, 2] -> [[0], [2]]\n",
    "targets_unsqueezed = targets.unsqueeze(1) \n",
    "\n",
    "selected_log_probs = torch.gather(\n",
    "    input=log_softmax_logits, # 로그 확률값들 중에서\n",
    "    dim=1,                    # 가로 방향(클래스 축)에서 찾는다\n",
    "    index=targets_unsqueezed  # 정답 인덱스에 해당하는 값만 가져와라\n",
    ")\n",
    "# 결과 Shape: (2, 1) -> [[첫번째 샘플 정답확률], [두번째 샘플 정답확률]]\n",
    "\n",
    "# (3) 차원 축소 (보기 편하게)\n",
    "selected_log_probs = selected_log_probs.squeeze(1)  # Shape: (2,)\n",
    "\n",
    "# (4) 최종 Loss 계산 (Negative Log Likelihood)\n",
    "# 로그 확률은 항상 음수(0~1 사이 값의 로그)이므로, 양수 Loss로 만들기 위해 마이너스(-)를 붙입니다.\n",
    "# 그리고 배치 전체의 평균(.mean())을 구합니다.\n",
    "manual_loss = -selected_log_probs.mean()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. PyTorch 내장 함수 사용 (실전용)\n",
    "# =========================================================\n",
    "\n",
    "# F.cross_entropy는 위 2번의 모든 과정(LogSoftmax + Gather + NLL)을 한 번에 처리합니다.\n",
    "# 수치적으로 더 안정적이고(Numerical Stability) 속도도 빠릅니다.\n",
    "cross_entropy_loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "print(f\"수동 계산: {manual_loss.item()}\")\n",
    "print(f\"자동 계산: {cross_entropy_loss.item()}\")\n",
    "# 두 값은 정확히 일치합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d7add-f7ff-4a87-9193-7878c42bf0e7",
   "metadata": {
    "id": "f86d7add-f7ff-4a87-9193-7878c42bf0e7"
   },
   "source": [
    "- 위에서 두 구현이 동일함을 확인할 수 있습니다. 이제 `torch.gather`의 메커니즘을 좀 더 좁혀서 살펴봅시다.\n",
    "- 다음 두 텐서를 고려해 보세요:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "508db6ba-cc40-479f-a996-2250cf862388",
   "metadata": {
    "id": "508db6ba-cc40-479f-a996-2250cf862388"
   },
   "outputs": [],
   "source": [
    "t = torch.tensor(\n",
    "  [[1., 2.,],\n",
    "   [3., 4.]]\n",
    ")\n",
    "\n",
    "m = torch.tensor(\n",
    "  [[1, 1],\n",
    "   [0, 1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821cbf45-8fbb-47b7-bae8-6c3271e36979",
   "metadata": {
    "id": "821cbf45-8fbb-47b7-bae8-6c3271e36979"
   },
   "source": [
    "- 위에서 `t`는 우리가 선택하고자 하는 값을 가진 텐서이고, `m`은 선택 방법을 지정하는 마스크입니다.\n",
    " - 예를 들어, `m`의 첫 번째 행이 `[1, 1]`을 포함하므로, `t`의 첫 번째 행에서 인덱스 `1`에 해당하는 값인 2를 두 번 선택하게 됩니다.\n",
    " - `m`의 두 번째 행 `[0, 1]`은 `t`의 두 번째 행에서 인덱스 0과 1의 위치인 `3.`과 `4.`를 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fdN5q1YPAbM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fdN5q1YPAbM",
    "outputId": "e935e8ad-1519-4c4b-dbff-65adae0a15a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(input=t, dim=-1, index=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10eeaf4-f24b-4e79-916a-abedf74fe4a3",
   "metadata": {
    "id": "d10eeaf4-f24b-4e79-916a-abedf74fe4a3"
   },
   "source": [
    "- 다시 말해, `torch.gather`는 선택 함수입니다.\n",
    "- 앞서 손실을 계산할 때, 50,257개의 토큰 어휘(vocabulary) 중에서 올바른 토큰에 해당하는 로그 확률을 가져오기 위해 이 함수를 사용했습니다.\n",
    "- 여기서 \"올바른\" 토큰이란 응답 항목에 주어진 토큰들을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d10a43-ee5b-47ed-9d55-ddd96e66cf0b",
   "metadata": {
    "id": "d5d10a43-ee5b-47ed-9d55-ddd96e66cf0b"
   },
   "source": [
    "- 위의 `compute_logprobs` 함수와 관련하여, 여기서 `cross_entropy` 대신 `torch.gather`를 사용하는 이유는 좀 더 세밀한 제어가 가능하기 때문이지만, 본질적으로는 유사한 아이디어입니다.\n",
    "- 여기서 사용하는 `selection_mask`는 프롬프트와 패딩 토큰을 선택적으로 무시하기 위함입니다.\n",
    "- 이제 `compute_logprobs` 함수를 다음과 같이 사용하여 `compute_dpo_loss` 손실 함수의 입력값을 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa7a4db-eba0-47d8-ad6d-7b5e7676e318",
   "metadata": {
    "id": "dfa7a4db-eba0-47d8-ad6d-7b5e7676e318"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_dpo_loss_batch(batch, policy_model, reference_model, beta):\n",
    "    \"\"\"\n",
    "    데이터 배치 하나를 받아서 DPO 학습을 위한 손실(Loss)과 보상(Reward)을 계산합니다.\n",
    "    \n",
    "    과정:\n",
    "    1. 학습 모델(Policy)로 Chosen/Rejected 데이터의 로그 확률 계산 (그래디언트 필요 O)\n",
    "    2. 참조 모델(Reference)로 Chosen/Rejected 데이터의 로그 확률 계산 (그래디언트 필요 X)\n",
    "    3. 위 4가지 값을 공식에 넣어 최종 Loss 산출\n",
    "    \"\"\"\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 1. 학습 중인 모델 (Policy Model) 평가\n",
    "    # ==============================================================================\n",
    "    # 모델이 Chosen(선호) 응답을 생성할 로그 확률 계산\n",
    "    # policy_model(batch[\"chosen\"]) -> 모델의 Forward 실행 (Logits 출력)\n",
    "    policy_chosen_log_probas = compute_logprobs(\n",
    "        logits=policy_model(batch[\"chosen\"]),\n",
    "        labels=batch[\"chosen\"],\n",
    "        selection_mask=batch[\"chosen_mask\"] # 질문(Prompt) 부분은 계산에서 제외\n",
    "    )\n",
    "    \n",
    "    # 모델이 Rejected(비선호) 응답을 생성할 로그 확률 계산\n",
    "    policy_rejected_log_probas = compute_logprobs(\n",
    "        logits=policy_model(batch[\"rejected\"]),\n",
    "        labels=batch[\"rejected\"],\n",
    "        selection_mask=batch[\"rejected_mask\"]\n",
    "    )\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # 2. 참조 모델 (Reference Model) 평가\n",
    "    # ==============================================================================\n",
    "    # 참조 모델은 학습되지 않으므로(Frozen), 그래디언트 계산을 꺼서 메모리와 속도를 아낍니다.\n",
    "    with torch.no_grad():\n",
    "        # 참조 모델 입장에서의 Chosen 응답 확률\n",
    "        ref_chosen_log_probas = compute_logprobs(\n",
    "            logits=reference_model(batch[\"chosen\"]),\n",
    "            labels=batch[\"chosen\"],\n",
    "            selection_mask=batch[\"chosen_mask\"]\n",
    "        )\n",
    "        # 참조 모델 입장에서의 Rejected 응답 확률\n",
    "        ref_rejected_log_probas = compute_logprobs(\n",
    "            logits=reference_model(batch[\"rejected\"]),\n",
    "            labels=batch[\"rejected\"],\n",
    "            selection_mask=batch[\"rejected_mask\"]\n",
    "        )\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 3. DPO 손실 및 보상 계산\n",
    "    # ==============================================================================\n",
    "    # 위에서 구한 4가지 확률 값(Policy Chosen/Rejected, Ref Chosen/Rejected)을\n",
    "    # DPO 공식에 대입하여 최종 Loss를 구합니다.\n",
    "    loss, chosen_rewards, rejected_rewards = compute_dpo_loss(\n",
    "        model_chosen_logprobs=policy_chosen_log_probas,\n",
    "        model_rejected_logprobs=policy_rejected_log_probas,\n",
    "        reference_chosen_logprobs=ref_chosen_log_probas,\n",
    "        reference_rejected_logprobs=ref_rejected_log_probas,\n",
    "        beta=beta # KL 제약 강도 조절 하이퍼파라미터\n",
    "    )\n",
    "    \n",
    "    # loss: 역전파(Backpropagation)에 사용될 값\n",
    "    # rewards: 학습이 잘 되고 있는지 확인하기 위한 지표(Metrics)\n",
    "    return loss, chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28caafb-f378-4332-a142-3e0f9ef67fbb",
   "metadata": {
    "id": "b28caafb-f378-4332-a142-3e0f9ef67fbb"
   },
   "source": [
    "- 위 함수는 단일 배치에 대해 작동합니다. 예를 들면 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd74fcc4-4280-41e9-9a22-838e85c84ee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd74fcc4-4280-41e9-9a22-838e85c84ee4",
    "outputId": "65a70828-7dd2-4f72-ffec-45aeaf8afad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.6931, device='cuda:0'), tensor(0., device='cuda:0'), tensor(0., device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss = compute_dpo_loss_batch(batch, policy_model, reference_model, beta=0.1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17429cd-2a00-41c8-9f16-38b1c9a5179f",
   "metadata": {
    "id": "b17429cd-2a00-41c8-9f16-38b1c9a5179f"
   },
   "source": [
    "- 아래에서는 이 함수를 데이터 로더의 지정된 `num_batches` 만큼 작동하도록 확장합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e9ad5-c5de-4d1b-9e93-3918bf5d5302",
   "metadata": {
    "id": "682e9ad5-c5de-4d1b-9e93-3918bf5d5302"
   },
   "outputs": [],
   "source": [
    "def compute_dpo_loss_loader(data_loader, policy_model, reference_model, beta, num_batches=None):\n",
    "    \"\"\"\n",
    "    데이터 로더에 있는 데이터들을 순회하며 DPO 손실(Loss)과 보상(Reward)의 평균을 계산합니다.\n",
    "    주로 모델 학습 중 '검증(Validation)' 단계에서 사용됩니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 점수를 누적할 변수 초기화 (Accumulators)\n",
    "    # 전체 배치의 합계를 구한 뒤 나중에 평균을 낼 것입니다.\n",
    "    total_loss, total_chosen_rewards, total_rejected_rewards = 0., 0., 0.\n",
    "\n",
    "    # 예외 처리: 데이터가 텅 비어있으면 계산 불가 -> NaN(Not a Number) 반환\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    # 2. 평가할 배치의 총 개수 결정\n",
    "    elif num_batches is None:\n",
    "        # 사용자가 개수를 지정하지 않았으면 데이터 전체를 다 봅니다.\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # 사용자가 지정한 개수(num_batches)가 실제 데이터 개수보다 많으면 에러가 날 수 있으므로,\n",
    "        # 둘 중 작은 값(min)을 선택하여 안전하게 처리합니다.\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    # 3. 데이터 로더 순회 (Loop)\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        # 지정된 배치 개수만큼만 계산하고 멈춥니다.\n",
    "        if i < num_batches:\n",
    "            # (핵심) 앞서 정의한 함수를 호출하여 '현재 배치'에 대한 점수 계산\n",
    "            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n",
    "                batch=batch,\n",
    "                policy_model=policy_model,\n",
    "                reference_model=reference_model,\n",
    "                beta=beta\n",
    "            )\n",
    "\n",
    "            # (중요) 결과 누적 (.item() 사용)\n",
    "            # 텐서(Tensor) 상태로 더하면 그래디언트 정보까지 계속 쌓여 메모리가 터질 수 있습니다.\n",
    "            # .item()을 사용하여 순수 숫자(float) 값만 뽑아내서 더해야 합니다.\n",
    "            total_loss += loss.item()\n",
    "            total_chosen_rewards += chosen_rewards.item()\n",
    "            total_rejected_rewards += rejected_rewards.item()\n",
    "\n",
    "        else:\n",
    "            # 목표한 배치 수만큼 계산이 끝났으면 루프 탈출\n",
    "            break\n",
    "\n",
    "    # 4. 최종 평균(Mean) 계산\n",
    "    # (누적된 총합) / (배치 개수)\n",
    "    total_loss /= num_batches\n",
    "    total_chosen_rewards /= num_batches\n",
    "    total_rejected_rewards /= num_batches\n",
    "\n",
    "    return total_loss, total_chosen_rewards, total_rejected_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e4c09-d285-44d5-be12-d29769950cb6",
   "metadata": {
    "id": "852e4c09-d285-44d5-be12-d29769950cb6"
   },
   "source": [
    "- 왜 `num_batches`를 지정할까요? 이는 순전히 효율성 때문입니다 (매번 전체 데이터셋에 대해 손실을 계산하면 훈련 속도가 크게 느려지기 때문입니다)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca95b7-18fe-4076-9138-f70f21607b8c",
   "metadata": {
    "id": "2cca95b7-18fe-4076-9138-f70f21607b8c"
   },
   "source": [
    "- 마지막으로, 나중에 훈련 함수에서 사용할 편의 함수를 정의합니다. 이 `evaluate_dpo_loss_loader` 함수는 로깅 목적으로 훈련 및 검증 로더 모두에 대해 DPO 손실과 보상을 계산합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3d214ec-49ba-4bf0-ac80-f90fa0d832e9",
   "metadata": {
    "id": "c3d214ec-49ba-4bf0-ac80-f90fa0d832e9"
   },
   "outputs": [],
   "source": [
    "def evaluate_dpo_loss_loader(policy_model, reference_model, train_loader, val_loader, beta, eval_iter):\n",
    "    \"\"\"훈련 및 검증 데이터셋에 대한 DPO 손실을 계산합니다\"\"\"\n",
    "\n",
    "    policy_model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_chosen_rewards, train_rejected_rewards = compute_dpo_loss_loader(\n",
    "            data_loader=train_loader,\n",
    "            policy_model=policy_model,\n",
    "            reference_model=reference_model,\n",
    "            beta=beta,\n",
    "            num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_loss, val_chosen_rewards, val_rejected_rewards = compute_dpo_loss_loader(\n",
    "            data_loader=val_loader,\n",
    "            policy_model=policy_model,\n",
    "            reference_model=reference_model,\n",
    "            beta=beta,\n",
    "            num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "    res = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_chosen_reward\": train_chosen_rewards,\n",
    "        \"train_rejected_reward\": train_rejected_rewards,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_chosen_reward\": val_chosen_rewards,\n",
    "        \"val_rejected_reward\": val_rejected_rewards\n",
    "    }\n",
    "\n",
    "    policy_model.train()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95ed92-6743-4f13-8b91-0fbf2e540de1",
   "metadata": {
    "id": "6e95ed92-6743-4f13-8b91-0fbf2e540de1"
   },
   "source": [
    "- 이번 섹션에서 다룬 내용을 간단히 요약하면 다음과 같습니다:\n",
    "  - 흐름: 모델을 통한 `logits` 계산 $\\rightarrow$ 로짓으로부터 `compute_logprobs` $\\rightarrow$ 로그 확률로부터 `compute_dpo_loss` 계산\n",
    "  - 위 과정을 돕는 `compute_dpo_loss_batch` 함수\n",
    "  - `compute_dpo_loss_batch`를 데이터 로더에 적용하는 `compute_dpo_loss_loader` 유틸리티 함수\n",
    "  - 로깅을 위해 훈련 및 검증 세트 데이터 로더에 `compute_dpo_loss_batch`를 적용하는 `evaluate_dpo_loss_loader` 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a8f18-536e-4d83-a0d0-ac518a85f157",
   "metadata": {
    "id": "cb8a8f18-536e-4d83-a0d0-ac518a85f157"
   },
   "source": [
    "&nbsp;\n",
    "# 5) 모델 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b11d63d-3ddc-4070-9b2b-5ca0edb08d0c",
   "metadata": {
    "id": "4b11d63d-3ddc-4070-9b2b-5ca0edb08d0c"
   },
   "source": [
    "- 이전 섹션에서 DPO 손실 함수를 설정했으므로 이제 드디어 모델을 훈련할 수 있습니다.\n",
    "- 이 훈련 함수는 사전 훈련(pretraining) 및 지시 미세 조정(instruction finetuning)에 사용했던 것과 동일하며, 다음과 같은 사소한 차이점만 있습니다:\n",
    " - 교차 엔트로피(cross-entropy) 손실을 새로운 DPO 손실 함수로 교체합니다.\n",
    " - 또한 RLHF 및 DPO 맥락에서 훈련 진행 상황을 추적하는 데 일반적으로 사용되는 보상(rewards)과 보상 마진(reward margins)을 추적합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d4904-f819-4d62-bfb4-85cf28863683",
   "metadata": {
    "id": "820d4904-f819-4d62-bfb4-85cf28863683"
   },
   "source": [
    "- 훈련을 시작하기 전에 초기 손실과 보상을 출력해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f90d9325-77b2-417f-88ff-0a5174889413",
   "metadata": {
    "id": "f90d9325-77b2-417f-88ff-0a5174889413"
   },
   "outputs": [],
   "source": [
    "from previous_chapters import generate_and_print_sample\n",
    "# 대안:\n",
    "# from llms_from_scratch.ch04 import generate_text_simple\n",
    "\n",
    "\n",
    "def train_model_dpo_simple(\n",
    "    policy_model, reference_model, train_loader, val_loader,\n",
    "    optimizer, num_epochs, beta,\n",
    "    eval_freq, eval_iter, start_context, tokenizer\n",
    "):\n",
    "\n",
    "    # 손실과 본 토큰 수를 추적하기 위한 리스트 초기화\n",
    "    tracking = {\n",
    "        \"train_losses\": [],\n",
    "        \"train_chosen_rewards\": [],\n",
    "        \"train_rejected_rewards\": [],\n",
    "        \"val_losses\": [],\n",
    "        \"val_chosen_rewards\": [],\n",
    "        \"val_rejected_rewards\": [],\n",
    "        \"tokens_seen\": []\n",
    "    }\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # 메인 훈련 루프\n",
    "    for epoch in range(num_epochs):\n",
    "        policy_model.train()  # 모델을 훈련 모드로 설정\n",
    "\n",
    "        for batch in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()  # 이전 배치 반복의 손실 그래디언트 초기화\n",
    "\n",
    "            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n",
    "                batch=batch,\n",
    "                policy_model=policy_model,\n",
    "                reference_model=reference_model,\n",
    "                beta=beta\n",
    "            )\n",
    "\n",
    "            loss.backward()  # 손실 그래디언트 계산\n",
    "            optimizer.step()  # 손실 그래디언트를 사용하여 모델 가중치 업데이트\n",
    "\n",
    "            tokens_seen += batch[\"chosen\"].numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # 선택적 평가 단계\n",
    "            if global_step % eval_freq == 0:\n",
    "                res = evaluate_dpo_loss_loader(\n",
    "                    policy_model=policy_model,\n",
    "                    reference_model=reference_model,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    beta=beta,\n",
    "                    eval_iter=eval_iter\n",
    "                )\n",
    "                tracking[\"train_losses\"].append(res[\"train_loss\"])\n",
    "                tracking[\"train_chosen_rewards\"].append(res[\"train_chosen_reward\"])\n",
    "                tracking[\"train_rejected_rewards\"].append(res[\"train_rejected_reward\"])\n",
    "                tracking[\"val_losses\"].append(res[\"val_loss\"])\n",
    "                tracking[\"val_chosen_rewards\"].append(res[\"val_chosen_reward\"])\n",
    "                tracking[\"val_rejected_rewards\"].append(res[\"val_rejected_reward\"])\n",
    "                tracking[\"tokens_seen\"].append(tokens_seen)\n",
    "                train_reward_margin = res[\"train_chosen_reward\"] - res[\"train_rejected_reward\"]\n",
    "                val_reward_margin = res[\"val_chosen_reward\"] - res[\"val_rejected_reward\"]\n",
    "\n",
    "                print(\n",
    "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {res['train_loss']:.3f}, Val loss {res['val_loss']:.3f}, \"\n",
    "                    f\"Train reward margins {train_reward_margin:.3f}, \"\n",
    "                    f\"Val reward margins {val_reward_margin:.3f}\"\n",
    "                )\n",
    "\n",
    "        # 각 에포크 후 샘플 텍스트 출력\n",
    "        generate_and_print_sample(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=loss.device,\n",
    "            start_context=start_context\n",
    "        )\n",
    "\n",
    "    return tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d53210c5-6d9c-46b0-af22-ee875c2806c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d53210c5-6d9c-46b0-af22-ee875c2806c5",
    "outputId": "8b1d2b39-16c5-4b99-e920-5b33d3c0f34d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6931471824645996\n",
      "Validation loss: 0.6931471824645996\n",
      "Train reward margin: 0.0\n",
      "Val reward margin: 0.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) # 데이터 로더의 셔플링 때문에 재현성을 위해 시드 설정\n",
    "\n",
    "res = evaluate_dpo_loss_loader(\n",
    "    policy_model=policy_model,\n",
    "    reference_model=reference_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    beta=0.1,\n",
    "    eval_iter=5\n",
    ")\n",
    "\n",
    "print(\"Training loss:\", res[\"train_loss\"])\n",
    "print(\"Validation loss:\", res[\"val_loss\"])\n",
    "\n",
    "print(\"Train reward margin:\", res[\"train_chosen_reward\"] - res[\"train_rejected_reward\"])\n",
    "print(\"Val reward margin:\", res[\"val_chosen_reward\"] - res[\"val_rejected_reward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a006e91-df94-43ca-8025-1ba791e37bc4",
   "metadata": {
    "id": "4a006e91-df94-43ca-8025-1ba791e37bc4"
   },
   "source": [
    "- 또한, 초기 모델의 응답 일부를 살펴보겠습니다 (검증 세트의 처음 3개 예시):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "q4Ro9DrBa7zH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4Ro9DrBa7zH",
    "outputId": "b974d4bd-b92a-4a2a-bb7a-5a2a0d1eca11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "Correct response:\n",
      ">> The meal is cooked by the chef every day.\n",
      "\n",
      "Model response:\n",
      ">> The meal is cooked every day by the chef.\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Classify an input string as either a noun or a verb.\n",
      "\n",
      "### Input:\n",
      "Dance\n",
      "\n",
      "Correct response:\n",
      ">> 'Dance' can be classified as a verb.\n",
      "\n",
      "Model response:\n",
      ">> Dance is a verb.\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a metaphor.\n",
      "\n",
      "### Input:\n",
      "The book is very interesting.\n",
      "\n",
      "Correct response:\n",
      ">> The book is a page-turner.\n",
      "\n",
      "Model response:\n",
      ">> The book is like a treasure.\n",
      "\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in val_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"\\n-------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2386ae-5c4c-448e-bfbf-4ec0604b171e",
   "metadata": {
    "id": "ac2386ae-5c4c-448e-bfbf-4ec0604b171e"
   },
   "source": [
    "- 위에서 원래 모델의 응답을 볼 수 있습니다.\n",
    "- DPO의 목표는 약간의 스타일 변화를 유도하는 것입니다. 이는 모델이 유사하지만 조금 더 정중한 응답을 생성하도록 하려는 것입니다.\n",
    "- 훈련을 시작하는 다음 코드 셀을 실행하기 전에, 몇 가지 설정에 대한 참고 사항입니다:\n",
    " - `AdamW` 옵티마이저에는 정책 모델의 파라미터만 전달합니다. 이는 우리가 최적화하려는 모델입니다 (참조 모델은 수정하고 싶지 않습니다).\n",
    " - 1 에포크만 훈련합니다. DPO는 붕괴(collapse)하기 매우 쉽기 때문입니다 (손실은 개선될 수 있지만, 모델이 엉뚱한 텍스트를 생성하기 시작할 수 있습니다).\n",
    " - DPO에서는 매우 작은 학습률(learning rate)을 사용하는 것이 좋습니다.\n",
    " - beta 값은 0.1에서 0.5로 늘려 DPO의 효과를 줄일 수 있습니다 (여기서는 결과를 더 눈에 띄게 만들기 위해 0.1을 사용합니다).\n",
    " - 훈련은 A100 GPU에서 약 2분이 소요되지만, 더 작은 L4 GPU에서도 4분 안에 훈련할 수 있습니다. M3 MacBook Air에서의 훈련은 약 30분이 걸립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54b739be-871e-4c97-bf14-ffd2c58e1311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54b739be-871e-4c97-bf14-ffd2c58e1311",
    "outputId": "d98b08b0-c325-411e-a1a4-05e7403f0345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.692, Val loss 0.693, Train reward margins 0.014, Val reward margins 0.006\n",
      "Ep 1 (Step 000005): Train loss 0.691, Val loss 0.692, Train reward margins 0.044, Val reward margins 0.031\n",
      "Ep 1 (Step 000010): Train loss 0.688, Val loss 0.690, Train reward margins 0.114, Val reward margins 0.055\n",
      "Ep 1 (Step 000015): Train loss 0.682, Val loss 0.688, Train reward margins 0.227, Val reward margins 0.094\n",
      "Ep 1 (Step 000020): Train loss 0.682, Val loss 0.686, Train reward margins 0.230, Val reward margins 0.149\n",
      "Ep 1 (Step 000025): Train loss 0.674, Val loss 0.682, Train reward margins 0.394, Val reward margins 0.225\n",
      "Ep 1 (Step 000030): Train loss 0.677, Val loss 0.677, Train reward margins 0.330, Val reward margins 0.333\n",
      "Ep 1 (Step 000035): Train loss 0.668, Val loss 0.672, Train reward margins 0.526, Val reward margins 0.434\n",
      "Ep 1 (Step 000040): Train loss 0.674, Val loss 0.668, Train reward margins 0.417, Val reward margins 0.531\n",
      "Ep 1 (Step 000045): Train loss 0.652, Val loss 0.662, Train reward margins 0.899, Val reward margins 0.674\n",
      "Ep 1 (Step 000050): Train loss 0.659, Val loss 0.657, Train reward margins 0.784, Val reward margins 0.803\n",
      "Ep 1 (Step 000055): Train loss 0.644, Val loss 0.653, Train reward margins 1.077, Val reward margins 0.897\n",
      "Ep 1 (Step 000060): Train loss 0.654, Val loss 0.649, Train reward margins 0.889, Val reward margins 0.988\n",
      "Ep 1 (Step 000065): Train loss 0.628, Val loss 0.645, Train reward margins 1.476, Val reward margins 1.074\n",
      "Ep 1 (Step 000070): Train loss 0.616, Val loss 0.641, Train reward margins 1.798, Val reward margins 1.182\n",
      "Ep 1 (Step 000075): Train loss 0.611, Val loss 0.637, Train reward margins 1.923, Val reward margins 1.281\n",
      "Ep 1 (Step 000080): Train loss 0.583, Val loss 0.634, Train reward margins 2.576, Val reward margins 1.352\n",
      "Ep 1 (Step 000085): Train loss 0.606, Val loss 0.630, Train reward margins 2.026, Val reward margins 1.439\n",
      "Ep 1 (Step 000090): Train loss 0.647, Val loss 0.626, Train reward margins 1.057, Val reward margins 1.535\n",
      "Ep 1 (Step 000095): Train loss 0.593, Val loss 0.623, Train reward margins 2.341, Val reward margins 1.608\n",
      "Ep 1 (Step 000100): Train loss 0.588, Val loss 0.621, Train reward margins 2.465, Val reward margins 1.678\n",
      "Ep 1 (Step 000105): Train loss 0.575, Val loss 0.619, Train reward margins 2.873, Val reward margins 1.732\n",
      "Ep 1 (Step 000110): Train loss 0.598, Val loss 0.615, Train reward margins 2.252, Val reward margins 1.824\n",
      "Ep 1 (Step 000115): Train loss 0.610, Val loss 0.613, Train reward margins 1.911, Val reward margins 1.897\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Rewrite the sentence using a metaphor.  ### Input: The book is very interesting.  ### Response: The book is filled with ideas.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the Netherlands?  ###\n",
      "Training completed in 2.53 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(policy_model.parameters(), lr=5e-6, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 1\n",
    "tracking = train_model_dpo_simple(\n",
    "    policy_model=policy_model,\n",
    "    reference_model=reference_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    beta=0.1, # 0.1 에서 0.5 사이의 값\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=format_input(val_data[2]),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8ea88-8771-4eb9-855d-2fe1ca2dc2fa",
   "metadata": {
    "id": "eba8ea88-8771-4eb9-855d-2fe1ca2dc2fa"
   },
   "source": [
    "- 위에서 추적된 결과를 통해 알 수 있듯이 손실이 개선되었습니다.\n",
    "- 또한, 선택된 응답과 거부된 응답의 보상 차이인 보상 마진(reward margins)도 개선되었으며, 이는 좋은 징조입니다.\n",
    "- 다음 섹션에서 이 결과들을 좀 더 구체적으로 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e23989-92bd-4ac2-a4bc-65d4c7ac334e",
   "metadata": {
    "id": "11e23989-92bd-4ac2-a4bc-65d4c7ac334e"
   },
   "source": [
    "&nbsp;\n",
    "# 6) 결과 분석하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d7d5fe-c617-45cb-8ea9-ddc7baa22654",
   "metadata": {
    "id": "66d7d5fe-c617-45cb-8ea9-ddc7baa22654"
   },
   "source": [
    "- DPO 손실을 플로팅하여 결과 분석을 시작해 봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ddcc66f-cd7c-4f46-96ea-af919ea1a199",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "8ddcc66f-cd7c-4f46-96ea-af919ea1a199",
    "outputId": "c7164b26-8d32-41d1-8c6a-ab835d58d4c5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_losses\n",
    "# 대안:\n",
    "# from llms_from_scratch.ch05 import plot_losses\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(tracking[\"train_losses\"]))\n",
    "plot_losses(\n",
    "    epochs_seen=epochs_tensor,\n",
    "    tokens_seen=tracking[\"tokens_seen\"],\n",
    "    train_losses=tracking[\"train_losses\"],\n",
    "    val_losses=tracking[\"val_losses\"],\n",
    "    label=\"loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bc233-895f-46d5-8e01-202b991cd60c",
   "metadata": {
    "id": "7f8bc233-895f-46d5-8e01-202b991cd60c"
   },
   "source": [
    "- 위에서 볼 수 있듯이 손실이 계속 개선되고 있으며, 이는 좋은 신호입니다.\n",
    "- 하향 곡선을 보면 모델을 조금 더 훈련시키고 싶은 유혹이 들 수 있습니다 (독자분들도 시도해 보시길 권장합니다). 하지만 DPO는 모델이 엉뚱한 응답을 생성하기 시작하는 붕괴(collapse) 현상이 발생하기 쉽다는 점을 유의하세요.\n",
    "- 다음으로, 보상 마진을 살펴보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dmbq6ruuf0Cl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "dmbq6ruuf0Cl",
    "outputId": "c2886c16-57da-41bd-c9f0-e936da9d9e4d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_reward_margins = [i-j for i,j in zip(tracking[\"train_chosen_rewards\"], tracking[\"train_rejected_rewards\"])]\n",
    "val_reward_margins = [i-j for i,j in zip(tracking[\"val_chosen_rewards\"], tracking[\"val_rejected_rewards\"])]\n",
    "\n",
    "plot_losses(\n",
    "    epochs_seen=epochs_tensor,\n",
    "    tokens_seen=tracking[\"tokens_seen\"],\n",
    "    train_losses=train_reward_margins,\n",
    "    val_losses=val_reward_margins,\n",
    "    label=\"reward margins\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69756011-acd6-404c-a5fc-7fe252cf20c8",
   "metadata": {
    "id": "69756011-acd6-404c-a5fc-7fe252cf20c8"
   },
   "source": [
    "- 보시다시피 의도한 대로 보상 마진이 개선됩니다. 이는 손실 곡선과 거울상을 이루며 좋은 신호입니다.\n",
    "- DPO 손실과 보상 마진은 훈련 중에 추적해야 할 중요한 지표이지만, 전체 이야기를 다 말해주지는 않습니다.\n",
    "- 마지막으로, 가장 중요한 것은 응답에 대한 정성적인 확인을 수행해야 한다는 점입니다.\n",
    "- 여기서는 응답을 직접 살펴볼 것입니다 (추가로 7장과 유사하게 LLM을 사용하여 응답에 점수를 매길 수도 있습니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5EfUXJGOali8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EfUXJGOali8",
    "outputId": "7ec7db47-d775-4646-f660-0d7f7e7c8503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "Correct response:\n",
      ">> The meal is cooked by the chef every day.\n",
      "\n",
      "Reference model response:\n",
      ">> The meal is cooked every day by the chef.\n",
      "\n",
      "Policy model response:\n",
      ">> The active sentence could be replaced by the passive sentence: 'The chef prepares the meal every day.'\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Classify an input string as either a noun or a verb.\n",
      "\n",
      "### Input:\n",
      "Dance\n",
      "\n",
      "Correct response:\n",
      ">> 'Dance' can be classified as a verb.\n",
      "\n",
      "Reference model response:\n",
      ">> Dance is a verb.\n",
      "\n",
      "Policy model response:\n",
      ">> The input 'Dance' could be classified as a verb.\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a metaphor.\n",
      "\n",
      "### Input:\n",
      "The book is very interesting.\n",
      "\n",
      "Correct response:\n",
      ">> The book is a page-turner.\n",
      "\n",
      "Reference model response:\n",
      ">> The book is like a treasure.\n",
      "\n",
      "Policy model response:\n",
      ">> The book is filled with ideas.\n",
      "\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in val_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=reference_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    reference_response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=policy_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    policy_response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nReference model response:\\n>> {reference_response_text.strip()}\")\n",
    "    print(f\"\\nPolicy model response:\\n>> {policy_response_text.strip()}\")\n",
    "    print(\"\\n-------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RmcKVg0JlHVF",
   "metadata": {
    "id": "RmcKVg0JlHVF"
   },
   "source": [
    "- 위의 참조 모델(reference model)과 정책 모델(policy model)의 응답을 보면, 최적화된 모델(즉, 정책 모델)이 원래 모델(즉, 참조 모델)에 비해 스타일이 약간 변경되었음을 실제로 확인할 수 있습니다.\n",
    "- 예를 들어, `\"Dance\" can be classified as a verb.`가 `The input string \"Dance\" could be classified as a verb.`로 변경되었는데, 이는 약간 더 정중한 응답입니다 (\"can\" 대신 \"could\"를 사용하여 주장을 덜 단정적이고 더 조심스럽게 표현했습니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "jJSwb2hzQwdP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJSwb2hzQwdP",
    "outputId": "6e755db4-9524-42a8-a58b-2218bf03e39a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Reference model response:\n",
      ">> The car is as fast as a bullet.\n",
      "\n",
      "Policy model response:\n",
      ">> The car would be very fast.\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Reference model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "\n",
      "Policy model response:\n",
      ">> The type of cloud typically associated with thunderstorms is a cumulus.\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Reference model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Policy model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=reference_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    reference_response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=policy_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    policy_response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nReference model response:\\n>> {reference_response_text.strip()}\")\n",
    "    print(f\"\\nPolicy model response:\\n>> {policy_response_text.strip()}\")\n",
    "    print(\"\\n-------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59984ad7-8c95-4021-a0d2-2c816a69a884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

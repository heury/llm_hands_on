# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).
# Source for "Build a Large Language Model From Scratch"
#   - https://www.manning.com/books/build-a-large-language-model-from-scratch
# Code: https://github.com/rasbt/LLMs-from-scratch

import argparse
import numpy as np
import matplotlib.pyplot as plt


#####################################
# Gated DeltaNet ì„¤ëª…
#####################################
# Gated DeltaNetì€ Qwen3-Nextì™€ Kimi Linearì—ì„œ ì‚¬ìš©í•˜ëŠ” ì„ í˜• ì–´í…ì…˜(Linear Attention) ê¸°ë²•ì…ë‹ˆë‹¤.
# ê¸°ì¡´ Transformerì˜ O(nÂ²) ë³µì¡ë„ë¥¼ O(n)ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.
#
# ============================================
# ì¼ë°˜ Attention vs Gated DeltaNet ë¹„êµ
# ============================================
#
# ì¼ë°˜ Attention (Quadratic):
#   ëª¨ë“  í† í°ì´ ëª¨ë“  í† í°ì„ ì°¸ì¡° â†’ O(nÂ²)
#   softmax(Q @ K.T) @ V â†’ nÃ—n ì–´í…ì…˜ í–‰ë ¬ ìƒì„±
#   KV ìºì‹œ: batch Ã— n_tokens Ã— n_heads Ã— d_head Ã— 2
#
# Gated DeltaNet (Linear):
#   í† í°ì„ í•˜ë‚˜ì”© ìˆœì°¨ ì²˜ë¦¬ â†’ O(n)
#   ê³ ì • í¬ê¸° ë©”ëª¨ë¦¬ ìƒíƒœ(S)ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” RNN ë°©ì‹
#   ìƒíƒœ: batch Ã— n_heads Ã— d_head Ã— d_head (ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ë¬´ê´€!)
#
# ============================================
# í•µì‹¬ ì•„ì´ë””ì–´: Delta Rule + Gating
# ============================================
#
#         ì…ë ¥ x
#            â”‚
#  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#  â–¼         â–¼         â–¼         â–¼
# W_Q       W_K       W_V      W_gate
#  â”‚         â”‚         â”‚         â”‚
#  â–¼         â–¼         â–¼         â–¼
#  Q         K         V       gate
#  â”‚         â”‚         â”‚
#  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
#       â”‚         â”‚
#       â–¼         â–¼
#  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#  â”‚  Delta Rule ìƒíƒœ   â”‚  â† ê³ ì • í¬ê¸° ë©”ëª¨ë¦¬ S (d_head Ã— d_head)
#  â”‚  ì—…ë°ì´íŠ¸ (RNN)    â”‚
#  â”‚                    â”‚
#  â”‚  S = S Ã— Î± (decay) â”‚  â† Î±: ì´ì „ ë©”ëª¨ë¦¬ ì–¼ë§ˆë‚˜ ìœ ì§€?
#  â”‚  delta = (v - kv_mem) Ã— Î² â”‚  â† Î²: ìƒˆ ì •ë³´ ì–¼ë§ˆë‚˜ ë°˜ì˜?
#  â”‚  S = S + k Ã— delta â”‚
#  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#            â”‚
#            â–¼
#       RMSNorm + SiLU(gate)
#            â”‚
#            â–¼
#          ì¶œë ¥
#
# ============================================
# í•µì‹¬ ê²Œì´íŠ¸
# ============================================
# | ê²Œì´íŠ¸ | ì—­í•  |
# |--------|------|
# | Î± (decay gate) | ì´ì „ ë©”ëª¨ë¦¬ë¥¼ ì–¼ë§ˆë‚˜ ìœ ì§€í• ì§€ (ë§ê°ë¥ ) |
# | Î² (update gate) | ìƒˆ ì…ë ¥ì´ ìƒíƒœë¥¼ ì–¼ë§ˆë‚˜ ìˆ˜ì •í• ì§€ |
# | output gate | ìµœì¢… ì¶œë ¥ì„ ì–¼ë§ˆë‚˜ í†µê³¼ì‹œí‚¬ì§€ (SiLU) |
#
# ============================================
# í•µì‹¬ ì½”ë“œ (Delta Rule)
# ============================================
# S = zeros(batch, num_heads, head_dim, head_dim)  # ê³ ì • í¬ê¸° ìƒíƒœ
#
# for t in range(num_tokens):  # ì„ í˜• ë³µì¡ë„!
#     S = S * alpha_t                    # 1. ë©”ëª¨ë¦¬ ê°ì‡ 
#     kv_mem = (S * k_t).sum()           # 2. í˜„ì¬ í‚¤ë¡œ ë©”ëª¨ë¦¬ ì¡°íšŒ
#     delta = (v_t - kv_mem) * beta_t    # 3. Delta ê³„ì‚°: ìƒˆ ê°’ - ì˜ˆì¸¡ ê°’
#     S = S + k_t * delta                # 4. ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
#     y_t = (S * q_t).sum()              # 5. ì¿¼ë¦¬ë¡œ ì¶œë ¥ ìƒì„±
#
# ============================================
# KV ìºì‹œ ë©”ëª¨ë¦¬ ë¹„êµ (ì´ ìŠ¤í¬ë¦½íŠ¸ì˜ í•µì‹¬)
# ============================================
#
# MHA KV ìºì‹œ:
#   batch Ã— n_tokens Ã— n_heads Ã— d_head Ã— 2
#   â†’ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ì„ í˜• ì¦ê°€ ğŸ“ˆ
#
# DeltaNet ìƒíƒœ:
#   batch Ã— n_heads Ã— d_head Ã— d_head
#   â†’ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ë¬´ê´€! (ê³ ì •) ğŸ“Š
#
# | ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ | MHA KV ìºì‹œ | DeltaNet ìƒíƒœ |
# |--------------|-------------|---------------|
# | 1K tokens    | ì¦ê°€        | ê³ ì •          |
# | 100K tokens  | 100Ã— ì¦ê°€   | ë™ì¼          |
# | 1M tokens    | 1000Ã— ì¦ê°€  | ë™ì¼          |
#
# ============================================
# 3:1 í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡° (Qwen3-Next, Kimi Linear)
# ============================================
#
# Layer 0: DeltaNet (ì„ í˜•)
# Layer 1: DeltaNet (ì„ í˜•)
# Layer 2: DeltaNet (ì„ í˜•)
# Layer 3: Full Attention (ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì°¸ì¡°)
# Layer 4: DeltaNet
# Layer 5: DeltaNet
# Layer 6: DeltaNet
# Layer 7: Full Attention
# ...
#
# ì´ìœ : DeltaNetì€ ê³ ì • í¬ê¸° ë©”ëª¨ë¦¬(S)ë¡œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì••ì¶•í•˜ë¯€ë¡œ ì •ë³´ ì†ì‹¤ ê°€ëŠ¥.
#       ì¼ë¶€ ë ˆì´ì–´ì—ì„œ Full Attentionì„ ì‚¬ìš©í•´ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ë§ ëŠ¥ë ¥ ë³´ì™„.
#
# ============================================
# Trade-off
# ============================================
# | ì¥ì                         | ë‹¨ì                            |
# |-----------------------------|--------------------------------|
# | O(n) ì„ í˜• ë³µì¡ë„            | ì „ì—­ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ë§ ì œí•œ      |
# | KV ìºì‹œ ë¶ˆí•„ìš” (ê³ ì • ìƒíƒœ)  | RNNì²˜ëŸ¼ ë³‘ë ¬í™” ì–´ë ¤ì›€ (í•™ìŠµ ì‹œ)|
# | ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì      | ë©”ëª¨ë¦¬ ë³‘ëª© (ì •ë³´ ì••ì¶•)        |
#
# ============================================
# ì‚¬ìš©ì²˜
# ============================================
# - Qwen3-Next: Gated DeltaNet + Gated Attention (3:1)
# - Kimi Linear: KDA (ì±„ë„ë³„ ê²Œì´íŒ…) + MLA (3:1)

# ë°ì´í„° íƒ€ì…ë³„ ë°”ì´íŠ¸ ìˆ˜
DTYPE_BYTES = {
    "fp32": 4,
    "bf16": 2,
    "fp16": 2,
    "fp8": 1,
    "int8": 1,
}


def kv_bytes_total_mha(batch, context_length, emb_dim, n_layers, bytes_per_elem, n_heads):
    """
    MHA (Multi-Head Attention) KV ìºì‹œ ë©”ëª¨ë¦¬ ê³„ì‚°

    KV ìºì‹œ ê³µì‹:
      batch Ã— context_length Ã— n_heads Ã— d_head Ã— 2 Ã— bytes_per_elem
                                                   â†‘
                                              Kì™€ V ë‘ ê°œ

    ì˜ˆ: batch=1, context=1024, emb=2048, heads=16, bf16
      d_head = 2048 / 16 = 128
      per_layer = 1 Ã— 1024 Ã— 16 Ã— 128 Ã— 2 Ã— 2 = 8,388,608 bytes (8MB)

    â†’ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë¹„ë¡€í•˜ì—¬ ì„ í˜• ì¦ê°€!
    """
    d_head = emb_dim // n_heads
    per_layer = batch * context_length * n_heads * d_head * 2 * bytes_per_elem
    return per_layer * n_layers


def kv_bytes_total_deltanet_no_conv(batch, emb_dim, n_layers, bytes_per_elem, n_heads):
    """
    Gated DeltaNet ìƒíƒœ ë©”ëª¨ë¦¬ ê³„ì‚° (Convolutional mixing ì œì™¸ ë‹¨ìˆœ ë²„ì „)

    ìƒíƒœ S ê³µì‹:
      batch Ã— n_heads Ã— d_head Ã— d_head Ã— bytes_per_elem
                        â†‘        â†‘
                     d_head Ã— d_head ê³ ì • í¬ê¸° ìƒíƒœ í–‰ë ¬

    ì˜ˆ: batch=1, emb=2048, heads=16, bf16
      d_head = 2048 / 16 = 128
      per_layer = 1 Ã— 16 Ã— 128 Ã— 128 Ã— 2 = 524,288 bytes (0.5MB)

    â†’ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ë¬´ê´€! í•­ìƒ ê³ ì • í¬ê¸°!
    â†’ context_length íŒŒë¼ë¯¸í„°ê°€ ì—†ìŒì— ì£¼ëª©
    """
    d_head = emb_dim // n_heads
    per_layer = batch * n_heads * d_head * d_head * bytes_per_elem
    return per_layer * n_layers


def gb(x):
    return x / 1e9


def main():
    p = argparse.ArgumentParser(description="Memory vs. Context Length: MHA vs. DeltaNet (3:1 mix)")
    p.add_argument("--batch", type=int, default=1)
    p.add_argument("--emb_dim", type=int, default=2048)
    p.add_argument("--n_heads", type=int, default=16)
    p.add_argument("--n_layers", type=int, default=48)
    p.add_argument("--dtype", choices=DTYPE_BYTES.keys(), default="bf16")
    p.add_argument("--min_ctx", type=int, default=128)
    p.add_argument("--max_ctx", type=int, default=131_072)
    args = p.parse_args()

    step = 100
    ctx = np.arange(args.min_ctx, args.max_ctx + 1, step, dtype=int)
    bytes_per_elem = DTYPE_BYTES[args.dtype]

    ####################################################
    # 1) Full Attention (MHA) - ëª¨ë“  ë ˆì´ì–´ê°€ MHA
    ####################################################
    # KV ìºì‹œê°€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¼ ì„ í˜• ì¦ê°€
    # ê·¸ë˜í”„ì—ì„œ ê°€íŒŒë¥¸ ì§ì„ ìœ¼ë¡œ í‘œì‹œë¨
    mha_bytes = np.array([
        kv_bytes_total_mha(args.batch, int(t), args.emb_dim, args.n_layers,
                           bytes_per_elem, args.n_heads)
        for t in ctx
    ], dtype=float)

    ####################################################
    # 2) DeltaNet Only - ëª¨ë“  ë ˆì´ì–´ê°€ DeltaNet
    ####################################################
    # ìƒíƒœ SëŠ” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ë¬´ê´€í•˜ê²Œ ê³ ì •!
    # ê·¸ë˜í”„ì—ì„œ ìˆ˜í‰ì„ ìœ¼ë¡œ í‘œì‹œë¨
    dnet_bytes_const = kv_bytes_total_deltanet_no_conv(
        args.batch, args.emb_dim, args.n_layers,
        bytes_per_elem, args.n_heads
    )
    dnet_bytes = np.full_like(mha_bytes, fill_value=dnet_bytes_const, dtype=float)

    ####################################################
    # 3) 3:1 Hybrid - Qwen3-Next, Kimi Linear ë°©ì‹
    ####################################################
    # 48 ë ˆì´ì–´ ê¸°ì¤€:
    #   - MHA ë ˆì´ì–´: 48 / 4 = 12ê°œ (ë§¤ 4ë²ˆì§¸ ë ˆì´ì–´)
    #   - DeltaNet ë ˆì´ì–´: 48 - 12 = 36ê°œ (ë‚˜ë¨¸ì§€)
    #
    # ë©”ëª¨ë¦¬ = MHA KV ìºì‹œ (12 ë ˆì´ì–´) + DeltaNet ìƒíƒœ (36 ë ˆì´ì–´)
    #        = ì»¨í…ìŠ¤íŠ¸ ë¹„ë¡€ ë¶€ë¶„ + ê³ ì • ë¶€ë¶„
    #
    # ê·¸ë˜í”„ì—ì„œ ì™„ë§Œí•œ ì§ì„ ìœ¼ë¡œ í‘œì‹œë¨ (ê¸°ìš¸ê¸°ê°€ MHAì˜ 1/4)
    n_mha_layers = args.n_layers / 4      # 1/4ì€ Full Attention
    n_dnet_layers = args.n_layers - n_mha_layers  # 3/4ì€ DeltaNet
    mix_bytes = np.array([
        kv_bytes_total_mha(args.batch, int(t), args.emb_dim, n_mha_layers,
                           bytes_per_elem, args.n_heads)
        + kv_bytes_total_deltanet_no_conv(args.batch, args.emb_dim, n_dnet_layers,
                                          bytes_per_elem, args.n_heads)
        for t in ctx
    ], dtype=float)

    # Convert to GB
    mha_gb = gb(mha_bytes)
    dnet_gb = gb(dnet_bytes)
    mix_gb = gb(mix_bytes)

    # Plot
    fig, ax = plt.subplots(figsize=(7, 4.5))
    ax.plot(ctx, mha_gb, label="Full Attention (MHA) KV cache")
    ax.plot(ctx, dnet_gb, label="All Gated DeltaNet (no conv)")
    ax.plot(ctx, mix_gb, label="3:1 layer ratio (3 DeltaNet : 1 Full Attention)")

    ax.set_xlabel("Context length (number of tokens)")
    ax.set_ylabel("KV cache size (GB)")
    ax.grid(True, which="both", linestyle="--", linewidth=0.5, alpha=0.6)
    ax.legend()

    fig.tight_layout()
    plt.savefig("deltanet_memory_plot.pdf", dpi=160)
    plt.close(fig)


if __name__ == "__main__":
    main()

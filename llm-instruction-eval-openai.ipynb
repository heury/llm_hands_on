{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "<a href=\"https://sebastianraschka.com\">Sebastian Raschka</a>의 책 <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a>를 위한 보충 코드<br>\n",
    "<br>코드 저장소: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OpenAI API를 사용한 지시(Instruction) 응답 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128651b-f326-4232-a994-42f38b7ed520",
   "metadata": {},
   "source": [
    "- 이 노트북은 OpenAI의 GPT-4 API를 사용하여 지시 튜닝(instruction finetuned)된 LLM의 응답을 평가합니다. 평가는 생성된 모델 응답이 포함된 JSON 형식의 데이터셋을 기반으로 합니다. 예시는 다음과 같습니다:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"instruction\": \"What is the atomic number of helium?\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"The atomic number of helium is 2.\",               # <-- 테스트 세트에 정답(Target)으로 주어진 값\n",
    "    \"model 1 response\": \"\\nThe atomic number of helium is 2.0.\", # <-- LLM 모델 1의 응답\n",
    "    \"model 2 response\": \"\\nThe atomic number of helium is 3.\"    # <-- 2번째 LLM 모델의 응답\n",
    "},\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9212cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63610acc-db94-437f-8d38-e99dca0299cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai version: 1.30.3\n",
      "tqdm version: 4.66.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"openai\",  # OpenAI API\n",
    "        \"tqdm\",    # 진행률 표시줄(Progress bar)\n",
    "        \"python-dotenv\"  # .env 파일에서 환경 변수 로드\n",
    "        ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
   "metadata": {},
   "source": [
    "## OpenAI API 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
   "metadata": {},
   "source": [
    "- 먼저, OpenAI API가 올바르게 설정되었는지 테스트해보겠습니다.\n",
    "- 아직 계정이 없다면 https://platform.openai.com/ 에서 계정을 생성해야 합니다.\n",
    "- GPT-4 API는 무료가 아니므로 계정에 자금을 충전해야 합니다 (https://platform.openai.com/settings/organization/billing/overview 참조).\n",
    "- 이 노트북의 코드를 사용하여 실험을 실행하고 약 200개의 평가를 생성하는 데 드는 비용은 작성 시점 기준으로 약 $0.26(26센트)입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89343a84-0ddc-42fc-bf50-298a342b93c0",
   "metadata": {},
   "source": [
    "- 먼저 OpenAI API 비밀 키(secret key)를 제공해야 합니다. 키는 https://platform.openai.com/api-keys 에서 찾을 수 있습니다.\n",
    "- 이 키를 다른 사람과 공유하지 않도록 주의하세요.\n",
    "- 비밀 키(`\"sk-...\"`)를 이 폴더에 있는 `config.json` 파일에 추가하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI 클라이언트가 성공적으로 설정되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. .env 파일에 정의된 환경 변수를 시스템으로 로드합니다.\n",
    "load_dotenv()\n",
    "\n",
    "# 2. os.getenv를 사용해 환경 변수 값을 가져옵니다.\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 3. 클라이언트를 생성합니다.\n",
    "# (참고: OpenAI 라이브러리는 환경 변수 'OPENAI_API_KEY'가 로드되어 있으면 \n",
    "#  api_key 인자를 생략해도 자동으로 감지합니다.)\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"OpenAI 클라이언트가 성공적으로 설정되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876",
   "metadata": {},
   "source": [
    "- 먼저, 의도한 대로 작동하는지 확인하기 위해 간단한 예제로 API를 테스트해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e9ef2e-e816-4283-840e-43625791ad33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_chatgpt(prompt, client, model=\"gpt-5-mini\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "prompt = \"Respond with 'hello world' if you got this message.\"\n",
    "run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
   "metadata": {},
   "source": [
    "## JSON 항목 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca011a8b-20c5-4101-979e-9b5fccf62f8a",
   "metadata": {},
   "source": [
    "- 여기서는 테스트 데이터셋과 모델 응답을 JSON 파일로 저장했다고 가정하고, 이를 다음과 같이 로드합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 110\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_file = \"outputs/instruction-data-with-response.json\"\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66",
   "metadata": {},
   "source": [
    "- 이 파일의 구조는 다음과 같으며, 테스트 데이터셋의 정답(`'output'`)과 두 가지 다른 모델의 응답(`'model 1 response'` 및 `'model 2 response'`)이 포함되어 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7222fdc0-5684-4f2b-b741-3e341851359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Rewrite the sentence using a simile.',\n",
       " 'input': 'The car is very fast.',\n",
       " 'output': 'The car is as fast as lightning.',\n",
       " 'model_response': 'The car is as fast as a bullet.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0331b-6024-4bba-89a9-a088b14a1046",
   "metadata": {},
   "source": [
    "- 아래는 시각화를 위해 입력을 포맷팅하는 작은 유틸리티 함수입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a55283-7d51-4136-ba60-f799d49f4098",
   "metadata": {},
   "source": [
    "- 이제 OpenAI API를 사용하여 모델 응답을 비교해 보겠습니다(시각적 비교를 위해 처음 5개 응답만 평가합니다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "735cc089-d127-480a-b39d-0782581f0c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "\n",
      "Score:\n",
      ">> 100\n",
      "\n",
      "Explanation: The response correctly rewrites the sentence as a simile (\"as fast as a bullet\"), is grammatical, and preserves the intended meaning (extreme speed). It's an acceptable alternative to \"as fast as lightning.\"\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "\n",
      "Score:\n",
      ">> 20\n",
      "\n",
      "Incorrect: thunderstorms are typically produced by cumulonimbus clouds. The answer \"cumulus\" is not specific (only cumulonimbus—an intense, vertically developed cumulus type—is the correct thunderstorm cloud), though cumulus congestus can develop into cumulonimbus, so minor partial credit.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> 100\n",
      "\n",
      "Reason: The response correctly and succinctly answers the question with the exact correct author (semantically equivalent to the expected \"Jane Austen\").\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in json_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", run_chatgpt(prompt, client))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
   "metadata": {},
   "source": [
    "- 응답이 매우 장황하다는 점을 유의하세요. 어떤 모델이 더 나은지 정량화하기 위해 점수만 반환받도록 하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552bdfb-7511-42ac-a9ec-da672e2a5468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  28%|██▊       | 31/110 [02:59<07:05,  5.39s/it]"
     ]
    }
   ],
   "source": [
    "def generate_model_scores(json_data, json_key, client=client):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = run_chatgpt(prompt, client)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = generate_model_scores(json_data, \"model_response\", client=client)\n",
    "print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71974dea-31ed-49af-abba-5c858bbbf49c",
   "metadata": {},
   "source": [
    "- 참고로 OpenAI의 GPT 모델은 랜덤 시드를 설정하더라도 비결정적(non-deterministic)이므로 응답 점수가 달라질 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-hands-on",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

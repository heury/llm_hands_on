{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6eb18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from transformer_lens import HookedTransformer\n",
    "# import pandas as pd\n",
    "\n",
    "# # 1. 모델 로드\n",
    "# model = HookedTransformer.from_pretrained(\"models/gpt2/gpt2-small-124M.pth\")\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "\n",
    "# 1. GPT-2 Small에 맞는 설정 생성\n",
    "cfg = HookedTransformerConfig.from_dict({\n",
    "    \"d_model\": 768,\n",
    "    \"d_head\": 64,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"d_mlp\": 3072,\n",
    "    \"act_fn\": \"gelu\",\n",
    "    \"tokenizer_name\": \"gpt2\",\n",
    "})\n",
    "\n",
    "# 2. 빈 모델 생성\n",
    "model = HookedTransformer(cfg)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 로컬 .pth 파일에서 가중치 로드\n",
    "state_dict = torch.load(r\"models\\gpt2\\gpt2-small-124M.pth\")\n",
    "\n",
    "# 4. 가중치 주입 (strict=False를 권장, 레이어 이름 형식이 다를 수 있음)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# 2. 분석할 프롬프트 설정 (Mary와 John 문제)\n",
    "prompt = \"When Mary and John went to the store, John gave a drink to\"\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "\n",
    "# 3. 마지막 토큰 위치에서 각 레이어의 잔차 연결(Residual Stream) 값 추출\n",
    "# [n_layers, pos, d_model] 형태의 데이터를 가져옵니다.\n",
    "accumulated_residual, labels = cache.accumulated_resid(pos_slice=-1, return_labels=True)\n",
    "\n",
    "# 4. 각 레이어의 결과물을 최종 출력(Logits)으로 변환\n",
    "# model.unembed()는 내부 벡터를 우리가 읽을 수 있는 단어 점수로 바꿔줍니다.\n",
    "logit_lens_logits = model.unembed(accumulated_residual) # [n_layers, d_vocab]\n",
    "\n",
    "# 5. 각 레이어별로 가장 확률이 높은 단어 TOP 1 추출\n",
    "top_tokens = torch.argmax(logit_lens_logits, dim=-1)\n",
    "decoded_tokens = [model.to_string(token) for token in top_tokens]\n",
    "\n",
    "# 결과 출력\n",
    "for i, token in enumerate(decoded_tokens):\n",
    "    print(f\"Layer {i:2}: 가장 유력한 예측 단어 -> '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b75211",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 모델 로드\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "\n",
    "# 2. 분석할 프롬프트 설정 (Mary와 John 문제)\n",
    "prompt = \"When Mary and John went to the store, John gave a drink to\"\n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "\n",
    "# 3. 마지막 토큰 위치에서 각 레이어의 잔차 연결(Residual Stream) 값 추출\n",
    "# [n_layers, pos, d_model] 형태의 데이터를 가져옵니다.\n",
    "accumulated_residual, labels = cache.accumulated_resid(pos_slice=-1, return_labels=True)\n",
    "\n",
    "# 4. 각 레이어의 결과물을 최종 출력(Logits)으로 변환\n",
    "# model.unembed()는 내부 벡터를 우리가 읽을 수 있는 단어 점수로 바꿔줍니다.\n",
    "logit_lens_logits = model.unembed(accumulated_residual) # [n_layers, d_vocab]\n",
    "\n",
    "# 5. 각 레이어별로 가장 확률이 높은 단어 TOP 1 추출\n",
    "top_tokens = torch.argmax(logit_lens_logits, dim=-1)\n",
    "decoded_tokens = [model.to_string(token) for token in top_tokens]\n",
    "\n",
    "# 결과 출력\n",
    "for i, token in enumerate(decoded_tokens):\n",
    "    print(f\"Layer {i:2}: 가장 유력한 예측 단어 -> '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9ab74b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m max_seq_length = \u001b[32m1024\u001b[39m \u001b[38;5;66;03m# Can increase for longer reasoning traces\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\unsloth\\__init__.py:95\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     84\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please update Unsloth and Unsloth-Zoo to the latest version!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     85\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDo this via `pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         )\n\u001b[32m     87\u001b[39m         \u001b[38;5;66;03m# if os.environ.get(\"UNSLOTH_DISABLE_AUTO_UPDATES\", \"0\") == \"0\":\u001b[39;00m\n\u001b[32m     88\u001b[39m         \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[32m     89\u001b[39m         \u001b[38;5;66;03m#         os.system(\"pip install --upgrade --no-cache-dir --no-deps unsloth_zoo\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m#         except:\u001b[39;00m\n\u001b[32m     94\u001b[39m         \u001b[38;5;66;03m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PackageNotFoundError:\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     98\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo` then retry!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\unsloth_zoo\\__init__.py:146\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m logging, torchao_logger, HideLoggingMessage\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Get device types and other variables\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdevice_type\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    147\u001b[39m     is_hip,\n\u001b[32m    148\u001b[39m     get_device_type,\n\u001b[32m    149\u001b[39m     DEVICE_TYPE,\n\u001b[32m    150\u001b[39m     DEVICE_TYPE_TORCH,\n\u001b[32m    151\u001b[39m     DEVICE_COUNT,\n\u001b[32m    152\u001b[39m     ALLOW_PREQUANTIZED_MODELS,\n\u001b[32m    153\u001b[39m )\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Torch 2.9 removed PYTORCH_HIP_ALLOC_CONF and PYTORCH_CUDA_ALLOC_CONF\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m major_torch == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m minor_torch >= \u001b[32m9\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\unsloth_zoo\\device_type.py:25\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m __all__ = [\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mis_hip\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mget_device_type\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mALLOW_BITSANDBYTES\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\__init__.py:2680\u001b[39m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n\u001b[32m   2679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[32m-> \u001b[39m\u001b[32m2680\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[32m   2682\u001b[39m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[32m   2683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mTORCH_CUDA_SANITIZER\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\_meta_registrations.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     13\u001b[39m     _add_op_to_registry,\n\u001b[32m     14\u001b[39m     _convert_out_params,\n\u001b[32m     15\u001b[39m     global_decomposition_table,\n\u001b[32m     16\u001b[39m     meta_table,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\_decomp\\__init__.py:276\u001b[39m\n\u001b[32m    272\u001b[39m             decompositions.pop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecompositions\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_refs\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcore_aten_decompositions\u001b[39m() -> \u001b[33m\"\u001b[39m\u001b[33mCustomDecompTable\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\_decomp\\decompositions.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_meta_registrations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprims\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\_prims\\__init__.py:525\u001b[39m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    521\u001b[39m \u001b[38;5;66;03m# Elementwise unary operations\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[38;5;28mabs\u001b[39m = \u001b[43m_make_elementwise_unary_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mabs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimpl_aten\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOMPLEX_TO_FLOAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m acos = _make_elementwise_unary_prim(\n\u001b[32m    533\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33macos\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    534\u001b[39m     impl_aten=torch.acos,\n\u001b[32m    535\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    536\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.DEFAULT,\n\u001b[32m    537\u001b[39m )\n\u001b[32m    539\u001b[39m acosh = _make_elementwise_unary_prim(\n\u001b[32m    540\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33macosh\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    541\u001b[39m     impl_aten=torch.acosh,\n\u001b[32m    542\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    543\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.DEFAULT,\n\u001b[32m    544\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\_prims\\__init__.py:493\u001b[39m, in \u001b[36m_make_elementwise_unary_prim\u001b[39m\u001b[34m(name, type_promotion, **kwargs)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_elementwise_unary_prim\u001b[39m(\n\u001b[32m    487\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m, *, type_promotion: ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND, **kwargs\n\u001b[32m    488\u001b[39m ):\n\u001b[32m    489\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[33;03m    Creates an elementwise unary prim.\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_make_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m(Tensor self) -> Tensor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_prim_elementwise_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRETURN_TYPE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNEW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\_prims\\__init__.py:321\u001b[39m, in \u001b[36m_make_prim\u001b[39m\u001b[34m(schema, return_type, meta, impl_aten, doc, tags, use_old_custom_ops_api, register_conj_neg_fallthrough)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m     mutates_args = [\n\u001b[32m    317\u001b[39m         arg.name\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m cpp_schema.arguments\n\u001b[32m    319\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m arg.alias_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m arg.alias_info.is_write\n\u001b[32m    320\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     prim_def = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcustom_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprims::\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_prim_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m     prim_def.register_fake(meta)\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# all view ops get conj/neg fallthroughs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:172\u001b[39m, in \u001b[36mcustom_op\u001b[39m\u001b[34m(name, fn, mutates_args, device_types, schema, tags)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:153\u001b[39m, in \u001b[36mcustom_op.<locals>.inner\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m    150\u001b[39m     schema_str = schema\n\u001b[32m    152\u001b[39m namespace, opname = name.split(\u001b[33m\"\u001b[39m\u001b[33m::\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m result = \u001b[43mCustomOpDef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# Check that schema's alias annotations match those of `mutates_args`.\u001b[39;00m\n\u001b[32m    156\u001b[39m     expected = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:211\u001b[39m, in \u001b[36mCustomOpDef.__init__\u001b[39m\u001b[34m(self, namespace, name, schema, fn, tags)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28mself\u001b[39m._autocast_cpu_dtype: Optional[_dtype] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._lib = get_library_allowing_overwrite(\u001b[38;5;28mself\u001b[39m._namespace, \u001b[38;5;28mself\u001b[39m._name)\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_register_to_dispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m._disabled_kernel: \u001b[38;5;28mset\u001b[39m = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    213\u001b[39m \u001b[38;5;28mself\u001b[39m._used_triton_kernels: \u001b[38;5;28mlist\u001b[39m[Any] = \u001b[38;5;28mlist\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\_library\\custom_ops.py:629\u001b[39m, in \u001b[36mCustomOpDef._register_to_dispatcher\u001b[39m\u001b[34m(self, tags)\u001b[39m\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    622\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThere was no fake impl registered for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    623\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis is necessary for torch.compile/export/fx tracing to work. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    624\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease use `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._init_fn.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.register_fake` to add an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    625\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfake impl.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    626\u001b[39m         )\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._abstract_fn(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_register_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m autograd_impl = autograd.make_autograd_impl(\u001b[38;5;28mself\u001b[39m._opoverload, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    632\u001b[39m lib.impl(\u001b[38;5;28mself\u001b[39m._name, autograd_impl, \u001b[33m\"\u001b[39m\u001b[33mAutograd\u001b[39m\u001b[33m\"\u001b[39m, with_keyset=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\library.py:190\u001b[39m, in \u001b[36mLibrary._register_fake\u001b[39m\u001b[34m(self, op_name, fn, _stacklevel, allow_override)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_register_fake\u001b[39m(\u001b[38;5;28mself\u001b[39m, op_name, fn, _stacklevel=\u001b[32m1\u001b[39m, *, allow_override=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    188\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Registers the fake impl for an operator defined in the library.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     source = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_library\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     frame = sys._getframe(_stacklevel)\n\u001b[32m    192\u001b[39m     caller_module = inspect.getmodule(frame)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\llm_hands_on\\.venv\\Lib\\site-packages\\torch\\_library\\utils.py:44\u001b[39m, in \u001b[36mget_source\u001b[39m\u001b[34m(stacklevel)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_source\u001b[39m(stacklevel: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a string that represents the caller.\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;03m    Example: \"/path/to/foo.py:42\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m \u001b[33;03m    etc.\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     frame = \u001b[43minspect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     source = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe.lineno\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m source\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.9-windows-x86_64-none\\Lib\\inspect.py:1696\u001b[39m, in \u001b[36mgetframeinfo\u001b[39m\u001b[34m(frame, context)\u001b[39m\n\u001b[32m   1694\u001b[39m start = lineno - \u001b[32m1\u001b[39m - context//\u001b[32m2\u001b[39m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1696\u001b[39m     lines, lnum = \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1698\u001b[39m     lines = index = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.9-windows-x86_64-none\\Lib\\inspect.py:1072\u001b[39m, in \u001b[36mfindsource\u001b[39m\u001b[34m(object)\u001b[39m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m (file.startswith(\u001b[33m'\u001b[39m\u001b[33m<\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file.endswith(\u001b[33m'\u001b[39m\u001b[33m>\u001b[39m\u001b[33m'\u001b[39m))) \u001b[38;5;129;01mor\u001b[39;00m file.endswith(\u001b[33m'\u001b[39m\u001b[33m.fwork\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33msource code not available\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m module = \u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module:\n\u001b[32m   1074\u001b[39m     lines = linecache.getlines(file, module.\u001b[34m__dict__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.9-windows-x86_64-none\\Lib\\inspect.py:1029\u001b[39m, in \u001b[36mgetmodule\u001b[39m\u001b[34m(object, _filename)\u001b[39m\n\u001b[32m   1026\u001b[39m         f = getabsfile(module)\n\u001b[32m   1027\u001b[39m         \u001b[38;5;66;03m# Always map to the name the module knows itself by\u001b[39;00m\n\u001b[32m   1028\u001b[39m         modulesbyfile[f] = modulesbyfile[\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m             \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m] = module.\u001b[34m__name__\u001b[39m\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m modulesbyfile:\n\u001b[32m   1031\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sys.modules.get(modulesbyfile[file])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen ntpath>:738\u001b[39m, in \u001b[36mrealpath\u001b[39m\u001b[34m(path, strict)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"openai-community/gpt2\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False, # False for LoRA 16bit\n",
    "    fast_inference = False, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.9, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beabdc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.2: Fast Gpt2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA A10-24Q. Num GPUs = 1. Max memory: 23.937 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n",
      "d:\\models\\openai-community\\gpt2-medium does not have a padding token! Will use pad_token = <|endoftext|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
    "lora_rank = 16 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = r\"d:\\models\\openai-community\\gpt2-medium\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False, # False for LoRA 16bit\n",
    "    fast_inference = False, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.9, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"c_attn\", \"attn.c_proj\", \"mlp.c_fc\", \"mlp.c_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fca20622",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    try:\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs       = examples[\"input\"]\n",
    "        outputs      = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing examples: {e}\")\n",
    "        texts = []\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func,batched=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cce175",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = dataset.column_names\n",
    "if \"text\" not in column_names:\n",
    "    print(f\"에러: 'text' 필드를 찾을 수 없습니다. 현재 필드: {column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bf83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda99937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 데이터셋 검증 시작 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46aa9a4c2dfe40cda2b0e61a4ed02628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=40):   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 100개 샘플 테스트 성공 ---\n"
     ]
    }
   ],
   "source": [
    "# 1. 함수 수정 (tokenizer 인자를 추가로 받음)\n",
    "def debugging_formatting_func(example, tokenizer): # tokenizer 추가\n",
    "    text = example['text']\n",
    "    \n",
    "    if text is None:\n",
    "        return {\"text\": \"\"} \n",
    "    \n",
    "    try:\n",
    "        # 인자로 받은 tokenizer 사용\n",
    "        tokenizer(text, truncation=True, max_length=1024)\n",
    "    except Exception as e:\n",
    "        print(f\"에러 발생 데이터: {text[:50]}...\")\n",
    "        raise e\n",
    "    \n",
    "    return example\n",
    "\n",
    "print(\"--- 데이터셋 검증 시작 ---\")\n",
    "\n",
    "try:\n",
    "    # 2. map 실행 시 fn_kwargs로 tokenizer 전달\n",
    "    small_dataset = dataset.select(range(min(0000, len(dataset)))) \n",
    "    small_dataset.map(\n",
    "        debugging_formatting_func, \n",
    "        fn_kwargs={\"tokenizer\": tokenizer}, # 여기서 전달!\n",
    "        num_proc=40,\n",
    "    )\n",
    "    print(\"--- 100개 샘플 테스트 성공 ---\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n치명적 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4c53cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='.\\models\\openai-community\\gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0eaccb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03c33af72394850b8a7eda21e62a9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. 밖에서 직접 토큰화 수행\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=128)\n",
    "\n",
    "# num_proc을 명시적으로 1로 주어 밖에서 처리\n",
    "dataset = dataset.map(tokenize_function,  fn_kwargs={\"tokenizer\": tokenizer},batched=True)\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    # dataset_text_field=\"text\",\n",
    "    dataset_text_field=None, # 미리 토큰화했으므로 None으로 설정\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=1,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",  # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e3ee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A10-24Q. Max memory = 23.937 GB.\n",
      "0.463 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56628ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 6,291,456 of 361,114,624 (1.74% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:41, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.444500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.534100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.900500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.837100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.741200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.623800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.501600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.487800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.979400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.851900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.766300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.830400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.854100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.906800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.812700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.839200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.739500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.660300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.633400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.720600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.773800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.546700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.523500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.676200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.587200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.650200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "595b47ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_gpu_memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#@title Show final memory and time stats\u001b[39;00m\n\u001b[32m      2\u001b[39m used_memory = \u001b[38;5;28mround\u001b[39m(torch.cuda.max_memory_reserved() / \u001b[32m1024\u001b[39m / \u001b[32m1024\u001b[39m / \u001b[32m1024\u001b[39m, \u001b[32m3\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m used_memory_for_lora = \u001b[38;5;28mround\u001b[39m(used_memory - \u001b[43mstart_gpu_memory\u001b[49m, \u001b[32m3\u001b[39m)\n\u001b[32m      4\u001b[39m used_percentage = \u001b[38;5;28mround\u001b[39m(used_memory         /max_memory*\u001b[32m100\u001b[39m, \u001b[32m3\u001b[39m)\n\u001b[32m      5\u001b[39m lora_percentage = \u001b[38;5;28mround\u001b[39m(used_memory_for_lora/max_memory*\u001b[32m100\u001b[39m, \u001b[32m3\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'start_gpu_memory' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0b8841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonnaci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\n1, 1, 2, 3, 5, 8\\n\\n### Instruction:\\nWrite a response that appropriately completes the request.\\n\\n### Input:\\n### Response:\\n### Instruction:\\nWrite a response that appropriately completes the request.\\n\\n### Input:\\n### Response:\\n### Instruction:\\nWrite']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76fca164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Mary and John went to the store, John gave a drink to Mary and said, \"Mary, I have a request for you.\"\n",
      "\n",
      "\"I have a request for you,\" Mary replied.\n",
      "\n",
      "\"I have a request for you,\" John replied.\n",
      "\n",
      "\"I have a request for you,\" Mary replied.\n",
      "\n",
      "\"I have a request for you,\" John replied.\n",
      "\n",
      "\"I have a request for you,\" Mary replied.\n",
      "\n",
      "\"I have a request for you,\" John replied.\n",
      "\n",
      "\"I have a request for you,\" Mary replied.\n",
      "\n",
      "\"I have a request for you,\" John replied.\n",
      "\n",
      "\"I have a request for you,\"\n"
     ]
    }
   ],
   "source": [
    "# 3. 입력 데이터 준비 (프롬프트 포맷팅)\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        \"When Mary and John went to the store, John gave a drink to\"\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# 4. 텍스트 생성\n",
    "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
    "\n",
    "# 5. 결과 디코딩\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen3-instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62decf67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-hands-on",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

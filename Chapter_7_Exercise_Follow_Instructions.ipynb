{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac1ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
    "# Source for \"Build a Large Language Model From Scratch\"\n",
    "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
    "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
    "#\n",
    "# 7장의 코드를 기반으로 한 최소한의 Instruction Finetuning 예제 파일\n",
    "\n",
    "from functools import partial\n",
    "from importlib.metadata import version\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 로컬 파일(이전 챕터에서 작성된 유틸리티 함수들)에서 필요한 모듈 임포트\n",
    "# GPTModel 구조, 생성 함수, 학습 루프 함수 등을 가져옵니다.\n",
    "from previous_chapters import (\n",
    "    calc_loss_loader,\n",
    "    generate,\n",
    "    GPTModel,\n",
    "    text_to_token_ids,\n",
    "    train_model_simple,\n",
    "    token_ids_to_text,\n",
    "    load_gpt2_model\n",
    ")\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    지시사항(Instruction) 데이터셋을 처리하는 PyTorch Dataset 클래스.\n",
    "    입력 데이터(지시+입력)와 정답 데이터(응답)를 하나의 텍스트로 합쳐서 토큰화합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # 텍스트 데이터 미리 토큰화 (Pre-tokenize)\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            # 포맷팅 함수를 이용해 \"지시사항 + 입력\" 텍스트 생성\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            # 정답(Response) 텍스트 생성\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            \n",
    "            # 모델은 이 전체 텍스트(질문+답변)를 보고 다음 토큰을 예측하도록 학습됨\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    데이터 로더에서 배치를 만들 때 사용하는 커스텀 함수.\n",
    "    가변 길이의 시퀀스를 배치의 최대 길이에 맞춰 패딩(padding)하고, \n",
    "    정답(target) 데이터에서 패딩 부분은 손실(loss) 계산에서 제외하도록 처리합니다.\n",
    "    \"\"\"\n",
    "    # 배치 내에서 가장 긴 시퀀스 길이 계산 (패딩을 위해 +1 여유)\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # 입력(inputs)과 정답(targets) 리스트 준비\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # 문장 끝 토큰 <|endoftext|> 추가\n",
    "        new_item += [pad_token_id]\n",
    "        \n",
    "        # 가장 긴 길이에 맞춰 패딩 추가\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        \n",
    "        # 입력은 마지막 토큰 제외 (0 ~ n-1)\n",
    "        inputs = torch.tensor(padded[:-1]) \n",
    "        # 정답은 첫 번째 토큰 제외 (1 ~ n) -> 다음 토큰 예측 문제이므로\n",
    "        targets = torch.tensor(padded[1:]) \n",
    "\n",
    "        # 중요: 패딩 부분 마스킹 처리\n",
    "        # 타겟에서 패딩 토큰인 부분은 ignore_index(-100)로 바꿔서 CrossEntropyLoss 계산 시 무시되게 함\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        \n",
    "        # 패딩이 시작되는 지점 이후의 모든 타겟 값을 -100으로 설정\n",
    "        # region [패딩 마스킹 처리]\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        # endregion\n",
    "        \n",
    "        # 선택적으로 최대 시퀀스 길이 제한 (메모리 관리 등 목적)\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # 텐서로 변환 및 디바이스(GPU/CPU)로 이동\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    \"\"\"\n",
    "    데이터 파일이 없으면 다운로드하고, JSON 형태로 로드하는 유틸리티 함수\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        text_data = response.text\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def format_input(entry):\n",
    "    \"\"\"\n",
    "    Alpaca 스타일의 프롬프트 템플릿을 적용하는 함수.\n",
    "    모델에게 역할을 부여하고 입력 형식을 통일합니다.\n",
    "    \"\"\"\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    # 추가적인 입력 정보(Context 등)가 있는 경우 추가\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    학습 진행 상황(Loss)을 그래프로 시각화하여 PDF로 저장하는 함수\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # 에폭(Epoch) 기준 Loss 그래프\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    # 처리한 토큰 수(Tokens seen) 기준 보조 x축 생성\n",
    "    ax2 = ax1.twiny() \n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0) \n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plot_name = \"outputs/loss-plot-standalone.pdf\"\n",
    "    print(f\"Plot saved as {plot_name}\")\n",
    "    plt.savefig(plot_name)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def main(test_mode=False):\n",
    "    #######################################\n",
    "    # 패키지 버전 출력 (디버깅용)\n",
    "    #######################################\n",
    "    print()\n",
    "    pkgs = [\n",
    "        \"matplotlib\",  # 시각화\n",
    "        \"tiktoken\",    # 토크나이저\n",
    "        \"torch\",       # 딥러닝 프레임워크\n",
    "        \"tqdm\",        # 진행률 표시 바\n",
    "    ]\n",
    "    for p in pkgs:\n",
    "        print(f\"{p} version: {version(p)}\")\n",
    "    print(50*\"-\")\n",
    "\n",
    "    #######################################\n",
    "    # 1. 데이터셋 다운로드 및 준비\n",
    "    #######################################\n",
    "    file_path = \"datas/instruction-data.json\"\n",
    "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "    data = download_and_load_file(file_path, url)\n",
    "\n",
    "    # 데이터 분할 (Train 85% / Test 10% / Valid 5%)\n",
    "    train_portion = int(len(data) * 0.85) \n",
    "    test_portion = int(len(data) * 0.1)   \n",
    "\n",
    "    train_data = data[:train_portion]\n",
    "    test_data = data[train_portion:train_portion + test_portion]\n",
    "    val_data = data[train_portion + test_portion:]\n",
    "\n",
    "    # 테스트 모드일 경우 아주 적은 데이터만 사용 (빠른 실행 확인용)\n",
    "    if test_mode:\n",
    "        train_data = train_data[:10]\n",
    "        val_data = val_data[:10]\n",
    "        test_data = test_data[:10]\n",
    "\n",
    "    print(\"Training set length:\", len(train_data))\n",
    "    print(\"Validation set length:\", len(val_data))\n",
    "    print(\"Test set length:\", len(test_data))\n",
    "    print(50*\"-\")\n",
    "\n",
    "    # 토크나이저 및 디바이스 설정\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "    print(50*\"-\")\n",
    "\n",
    "    # 커스텀 collate 함수 설정 (partial을 사용해 고정 인자 전달)\n",
    "    customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)\n",
    "\n",
    "    num_workers = 0\n",
    "    batch_size = 8\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    # 학습용 데이터 로더\n",
    "    train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=customized_collate_fn, # 커스텀 배치 처리 함수 사용\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    # 검증용 데이터 로더\n",
    "    val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=customized_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    #######################################\n",
    "    # 2. 사전 학습된 모델 로드 (Pre-trained Model)\n",
    "    #######################################\n",
    "\n",
    "    if test_mode:\n",
    "        # 테스트 모드용 초소형 더미 모델\n",
    "        BASE_CONFIG = {\n",
    "            \"vocab_size\": 50257,\n",
    "            \"context_length\": 120,\n",
    "            \"drop_rate\": 0.0,\n",
    "            \"qkv_bias\": False,\n",
    "            \"emb_dim\": 12,\n",
    "            \"n_layers\": 1,\n",
    "            \"n_heads\": 2\n",
    "        }\n",
    "        model = GPTModel(BASE_CONFIG)\n",
    "        model.eval()\n",
    "        device = \"cpu\"\n",
    "        CHOOSE_MODEL = \"Small test model\"\n",
    "\n",
    "    else:\n",
    "        # 실제 학습용 설정: GPT-2 Medium (355M)\n",
    "        BASE_CONFIG = {\n",
    "            \"vocab_size\": 50257,     \n",
    "            \"context_length\": 1024,  \n",
    "            \"drop_rate\": 0.0,        \n",
    "            \"qkv_bias\": True         \n",
    "        }\n",
    "\n",
    "        model_configs = {\n",
    "            \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "            \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "            \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "            \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "        }\n",
    "\n",
    "        CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "        BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "        # 로컬에 저장된 모델 가중치 파일 로드\n",
    "        model_name = \"gpt2-medium-355M.pth\"\n",
    "        model = load_gpt2_model(model_name, BASE_CONFIG)\n",
    "\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "    print(\"Loaded model:\", CHOOSE_MODEL)\n",
    "    print(50*\"-\")\n",
    "\n",
    "    #######################################\n",
    "    # 3. 모델 미세 조정 (Finetuning)\n",
    "    #######################################\n",
    "    print(\"Initial losses\")\n",
    "    # 학습 전 초기 손실값 확인\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "    print(\"   Training loss:\", train_loss)\n",
    "    print(\"   Validation loss:\", val_loss)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # 옵티마이저 설정 (AdamW)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "    num_epochs = 2\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    \n",
    "    # 실제 학습 수행 (previous_chapters.py에 정의된 함수 사용)\n",
    "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "        start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "\n",
    "    # 학습 결과 시각화\n",
    "    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "    print(50*\"-\")\n",
    "\n",
    "    #######################################\n",
    "    # 4. 결과 저장 및 테스트\n",
    "    #######################################\n",
    "    print(\"Generating responses\")\n",
    "    # 테스트 데이터셋에 대해 모델 응답 생성\n",
    "    for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "        input_text = format_input(entry)\n",
    "\n",
    "        # 모델 생성 (Inference)\n",
    "        token_ids = generate(\n",
    "            model=model,\n",
    "            idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "            max_new_tokens=256,\n",
    "            context_size=BASE_CONFIG[\"context_length\"],\n",
    "            eos_id=50256\n",
    "        )\n",
    "        generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        \n",
    "        # 프롬프트 부분을 제거하고 응답 부분만 추출\n",
    "        response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "        test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "    # 생성된 응답을 포함하여 JSON 파일로 저S장\n",
    "    test_data_path = \"outputs/instruction-data-with-response-standalone.json\"\n",
    "    with open(test_data_path, \"w\") as file:\n",
    "        json.dump(test_data, file, indent=4) \n",
    "    print(f\"Responses saved as {test_data_path}\")\n",
    "\n",
    "    # 미세 조정된 모델 가중치 저장\n",
    "    file_name = f\"outputs/{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft-standalone.pth\"\n",
    "    torch.save(model.state_dict(), file_name)\n",
    "    print(f\"Model saved as {file_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18669a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(test_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcc746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!killall -9 python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8282f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

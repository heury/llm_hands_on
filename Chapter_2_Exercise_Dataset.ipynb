{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac1fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "# region  [token encoding]\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "# endregion\n",
    "print(integers)\n",
    "\n",
    "# region  [token decoding]\n",
    "strings = tokenizer.decode(integers)\n",
    "# endregion\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e912c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.makedirs(\"datas\", exist_ok=True)\n",
    "if not os.path.exists(\"datas/the-verdict.txt\"):\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\"\n",
    "    )\n",
    "    file_path = \"datas/the-verdict.txt\"\n",
    "\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "with open(\"datas/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d50cdf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 중요: 이 코드는 GPT 모델 학습을 위한 데이터셋 클래스를 정의합니다\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt (str): 학습할 전체 텍스트 데이터\n",
    "            tokenizer: 텍스트를 토큰 ID로 변환해주는 토크나이저 (예: tiktoken)\n",
    "            max_length (int): 모델이 한 번에 볼 수 있는 윈도우 크기 (입력 시퀀스 길이)\n",
    "            stride (int): 윈도우를 이동시킬 간격 (데이터 중복 정도를 결정)\n",
    "        \"\"\"\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # 1. 전체 텍스트 토큰화\n",
    "        # 텍스트를 정수 리스트(token_ids)로 변환합니다.\n",
    "        # <|endoftext|> 같은 특수 토큰도 허용하여 인코딩합니다.\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # 데이터가 너무 짧으면 학습할 수 없으므로 최소 길이를 확인합니다.\n",
    "        assert len(token_ids) > max_length, \"토큰화된 입력의 개수는 적어도 max_length+1과 같아야 합니다.\"\n",
    "\n",
    "        # 2. 슬라이딩 윈도우(Sliding Window)로 데이터 생성\n",
    "        # 전체 토큰 리스트를 훑으며 max_length 길이만큼 잘라냅니다.\n",
    "        # stride만큼 건너뛰며 반복합니다.\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            # 입력 청크: 현재 위치(i)부터 max_length만큼 가져옵니다.\n",
    "            # region [input 청크 생성]\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            # endregion\n",
    "              \n",
    "            # 타겟 청크: 입력보다 1칸 뒤의 위치(i+1)부터 가져옵니다.\n",
    "            # GPT는 '다음 단어'를 맞추는 모델이므로, 정답은 입력보다 한 칸씩 뒤로 밀려있어야 합니다.\n",
    "            # region [target 청크 생성]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            # endregion\n",
    "\n",
    "            # 추출한 데이터를 텐서(Tensor)로 변환하여 리스트에 저장합니다.\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 총 샘플(청크) 개수를 반환합니다.\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # DataLoader가 데이터를 요청할 때 호출됩니다.\n",
    "        # 해당 인덱스(idx)의 입력과 정답 쌍을 반환합니다.\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022bae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "##중요 : 아래 함수는 DataLoader를 생성하는 헬퍼 함수입니다.\n",
    "import tiktoken\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # 토크나이저를 초기화합니다.\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # 데이터셋을 만듭니다.\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # 데이터 로더를 만듭니다.\n",
    "    # region [DataLoader 생성]\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    # endregion\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf379cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2f775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# region [임베딩 레이어 생성]\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "# endregion\n",
    "\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "# region [임베딩 레이어 적용]\n",
    "token_embeddings = embedding_layer(input_ids)\n",
    "# endregion\n",
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14451e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "context_length = max_length\n",
    "# region [포지셔널 임베딩 레이어 생성]\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "# endregion\n",
    "# region [포지셔널 임베딩 적용]\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "# endregion\n",
    "print(pos_embeddings.shape)\n",
    "# region [토큰 임베딩과 포지셔널 임베딩 합산]\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "# endregion\n",
    "print(input_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f4d74",
   "metadata": {},
   "source": [
    "## Tokenizer 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803265ab",
   "metadata": {},
   "source": [
    "- 아래 결과를 보면 한국어는 Exaone, 중국어는 Qwen의 Tokenizer 성능이 좋습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e6de65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0015db4095ca4b598a73a06149a225c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d212e42ddd94127b8658d586a26efcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd087569dd524c54834fc2a91db57d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d18928dd1a742d48cf1836201d8e74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cc796460ac45b88b59ebd11c22863f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d7fb85a24a4fa8a3fb3dc1625494f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7019bf25a4c944c5884533d3a9c63090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab31aac51bf4b698c2624cf660c2b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9306944df76f47d6acd8cf2bd5f01466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ac18ed28a84c5eadfc3eb63174ba6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "텍스트: 안녕하세요, 반갑습니다!\n",
      "------------------------------------------------------------\n",
      "Qwen3:  8개 토큰\n",
      "        ['안', '녕', '하세요', ',', ' 반', '갑', '습니다', '!']\n",
      "EXAONE: 7개 토큰\n",
      "        ['안녕', '하', '세요', ',', ' 반갑', '습니다', '!']\n",
      "→ EXAONE이 1개 더 효율적\n",
      "\n",
      "텍스트: Hello, nice to meet you!\n",
      "------------------------------------------------------------\n",
      "Qwen3:  7개 토큰\n",
      "        ['Hello', ',', ' nice', ' to', ' meet', ' you', '!']\n",
      "EXAONE: 7개 토큰\n",
      "        ['Hello', ',', ' nice', ' to', ' meet', ' you', '!']\n",
      "→ 동일\n",
      "\n",
      "텍스트: 인공지능 기술이 발전하고 있습니다.\n",
      "------------------------------------------------------------\n",
      "Qwen3:  12개 토큰\n",
      "        ['인', '공', '지', '능', ' 기', '술', '이', ' 발', '전', '하고']\n",
      "EXAONE: 9개 토큰\n",
      "        ['인공', '지능', ' 기술', '이', ' 발전', '하고', ' 있', '습니다', '.']\n",
      "→ EXAONE이 3개 더 효율적\n",
      "\n",
      "텍스트: Hello! This is an English tokenizer test.\n",
      "------------------------------------------------------------\n",
      "Qwen3:  9개 토큰\n",
      "        ['Hello', '!', ' This', ' is', ' an', ' English', ' tokenizer', ' test', '.']\n",
      "EXAONE: 9개 토큰\n",
      "        ['Hello', '!', ' This', ' is', ' an', ' English', ' tokenizer', ' test', '.']\n",
      "→ 동일\n",
      "\n",
      "텍스트: こんにちは！日本語のテストです。\n",
      "------------------------------------------------------------\n",
      "Qwen3:  8개 토큰\n",
      "        ['こんにちは', '！', '日本', '語', 'の', 'テスト', 'です', '。']\n",
      "EXAONE: 15개 토큰\n",
      "        ['こ', 'ん', 'に', 'ち', 'は', '�', '�', '�', '日本', '語']\n",
      "→ Qwen3가 7개 더 효율적\n",
      "\n",
      "텍스트: 人工智能技术正在快速发展。\n",
      "------------------------------------------------------------\n",
      "Qwen3:  5개 토큰\n",
      "        ['人工智能', '技术', '正在', '快速发展', '。']\n",
      "EXAONE: 17개 토큰\n",
      "        ['人', '工', '�', '�', '能', '技', '�', '�', '正', '在']\n",
      "→ Qwen3가 12개 더 효율적\n",
      "\n",
      "텍스트: The quick brown fox jumps over the lazy dog. 빠른 갈색 여우가 게으른 개를 뛰어넘습니다.\n",
      "------------------------------------------------------------\n",
      "Qwen3:  32개 토큰\n",
      "        ['The', ' quick', ' brown', ' fox', ' jumps', ' over', ' the', ' lazy', ' dog', '.']\n",
      "EXAONE: 20개 토큰\n",
      "        ['The', ' quick', ' brown', ' fox', ' jumps', ' over', ' the', ' lazy', ' dog', '.']\n",
      "→ EXAONE이 12개 더 효율적\n",
      "\n",
      "텍스트: 파이썬 프로그래밍은 매우 유용합니다. Python programming is very useful.\n",
      "------------------------------------------------------------\n",
      "Qwen3:  19개 토큰\n",
      "        ['파', '이', '썬', ' 프', '로그', '래', '밍', '은', ' 매우', ' 유']\n",
      "EXAONE: 14개 토큰\n",
      "        ['파이', '썬', ' 프로그래밍', '은', ' 매우', ' 유용', '합니다', '.', ' Python', ' programming']\n",
      "→ EXAONE이 5개 더 효율적\n",
      "\n",
      "텍스트: 123456789 !@#$%^&*() 특수문자\n",
      "------------------------------------------------------------\n",
      "Qwen3:  20개 토큰\n",
      "        ['1', '2', '3', '4', '5', '6', '7', '8', '9', ' !']\n",
      "EXAONE: 20개 토큰\n",
      "        ['1', '2', '3', '4', '5', '6', '7', '8', '9', ' !']\n",
      "→ 동일\n",
      "\n",
      "================================================================================\n",
      "4. Special Tokens 비교\n",
      "================================================================================\n",
      "\n",
      "[Qwen3 Special Tokens]\n",
      "  eos_token: '<|im_end|>' (ID: 151645)\n",
      "  pad_token: '<|endoftext|>' (ID: 151643)\n",
      "\n",
      "[EXAONE Special Tokens]\n",
      "  bos_token: '[BOS]' (ID: 1)\n",
      "  eos_token: '[|endofturn|]' (ID: 361)\n",
      "  unk_token: '[UNK]' (ID: 3)\n",
      "  pad_token: '[PAD]' (ID: 0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# Tokenizer 로드 (로컬 또는 온라인에서)\n",
    "print(\"Tokenizer 로드 중...\")\n",
    "qwen = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "exaone = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-1.2B\")\n",
    "\n",
    "# 테스트할 텍스트\n",
    "texts = [\n",
    "    \"안녕하세요, 반갑습니다!\",\n",
    "    \"Hello, nice to meet you!\",\n",
    "    \"인공지능 기술이 발전하고 있습니다.\",\n",
    "    \"Hello! This is an English tokenizer test.\",\n",
    "    \"こんにちは！日本語のテストです。\",\n",
    "    \"人工智能技术正在快速发展。\",\n",
    "    \"The quick brown fox jumps over the lazy dog. 빠른 갈색 여우가 게으른 개를 뛰어넘습니다.\",\n",
    "    \"파이썬 프로그래밍은 매우 유용합니다. Python programming is very useful.\",\n",
    "    \"123456789 !@#$%^&*() 특수문자\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "for text in texts:\n",
    "    print(f\"\\n텍스트: {text}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Qwen3\n",
    "    qwen_tokens = qwen.encode(text, add_special_tokens=True)\n",
    "    qwen_token_strs = [qwen.decode([tok]) for tok in qwen_tokens[:10]]\n",
    "    print(f\"Qwen3:  {len(qwen_tokens)}개 토큰\")\n",
    "    print(f\"        {qwen_token_strs}\")\n",
    "    \n",
    "    # EXAONE\n",
    "    exaone_tokens = exaone.encode(text, add_special_tokens=True)\n",
    "    exaone_token_strs = [exaone.decode([tok]) for tok in exaone_tokens[:10]]\n",
    "    print(f\"EXAONE: {len(exaone_tokens)}개 토큰\")\n",
    "    print(f\"        {exaone_token_strs}\")\n",
    "    \n",
    "    # 비교\n",
    "    diff = len(qwen_tokens) - len(exaone_tokens)\n",
    "    if diff > 0:\n",
    "        print(f\"→ EXAONE이 {diff}개 더 효율적\")\n",
    "    elif diff < 0:\n",
    "        print(f\"→ Qwen3가 {abs(diff)}개 더 효율적\")\n",
    "    else:\n",
    "        print(f\"→ 동일\")\n",
    "\n",
    "    # Special tokens 비교\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. Special Tokens 비교\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n[Qwen3 Special Tokens]\")\n",
    "for token_name in ['bos_token', 'eos_token', 'unk_token', 'pad_token']:\n",
    "    token = getattr(qwen, token_name, None)\n",
    "    if token:\n",
    "        token_id = getattr(qwen, f\"{token_name}_id\", None)\n",
    "        print(f\"  {token_name}: '{token}' (ID: {token_id})\")\n",
    "\n",
    "print(f\"\\n[EXAONE Special Tokens]\")\n",
    "for token_name in ['bos_token', 'eos_token', 'unk_token', 'pad_token']:\n",
    "    token = getattr(exaone, token_name, None)\n",
    "    if token:\n",
    "        token_id = getattr(exaone, f\"{token_name}_id\", None)\n",
    "        print(f\"  {token_name}: '{token}' (ID: {token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e4365",
   "metadata": {},
   "source": [
    "## Hugging Face 모델 파일 구조 설명\n",
    "\n",
    "Hugging Face Hub에 업로드된 파일들은 **Transformer 기반 언어 모델(LLM)**을 구성하는 핵심 요소들입니다. 크게 **설계도(Config)**, **가중치(Weights)**, **토크나이저(Tokenizer)** 세 그룹으로 나뉩니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 모델 아키텍처 및 설정 (설계도)\n",
    "\n",
    "* **`config.json`**\n",
    "    * **의미:** 모델의 **구조와 설계도**가 담긴 가장 중요한 설정 파일입니다.\n",
    "    * **내용:** 모델의 종류(Llama, BERT 등), 레이어(Layer) 수, 히든 사이즈, 어텐션 헤드 수 등을 정의하여 빈 모델(Skeleton)을 만듭니다.\n",
    "\n",
    "* **`generation_config.json`**\n",
    "    * **의미:** 텍스트 **생성(Generation) 시 사용할 기본 옵션** 파일입니다.\n",
    "    * **내용:** `max_length`(최대 길이), `temperature`(창의성), `top_p`, `eos_token_id`(종료 토큰) 등의 기본 설정값이 들어있습니다.\n",
    "\n",
    "### 2. 모델 가중치 (학습된 뇌)\n",
    "\n",
    "* **`model.safetensors`**\n",
    "    * **의미:** 실제 **학습된 파라미터(가중치)**가 저장된 파일입니다. (가장 용량이 큼)\n",
    "    * **특징:**\n",
    "        * 기존 PyTorch의 `.bin` 파일보다 로딩 속도가 빠릅니다.\n",
    "        * 보안 취약점(악성 코드 실행 등)을 해결한 안전한 형식입니다.\n",
    "\n",
    "### 3. 토크나이저 (언어 처리기)\n",
    "사람의 언어(Text)를 모델이 이해하는 숫자(Token ID)로 변환하는 규칙들입니다.\n",
    "\n",
    "* **`vocab.json`**\n",
    "    * **의미:** **단어장(Vocabulary)** 파일입니다. 단어와 그에 대응하는 숫자 ID가 매핑되어 있습니다.\n",
    "* **`merges.txt`**\n",
    "    * **의미:** **BPE 병합 규칙** 파일입니다. 알파벳 조각들이 어떻게 합쳐져서 단어가 되는지 정의합니다.\n",
    "* **`tokenizer.json`**\n",
    "    * **의미:** `vocab.json`과 `merges.txt` 등을 하나로 통합하여 최적화한 파일로, 로딩 속도가 매우 빠릅니다.\n",
    "* **`tokenizer_config.json`**\n",
    "    * **의미:** 토크나이저 설정 파일입니다. 어떤 클래스를 쓸지, 특수 토큰(`<s>`, `<pad>` 등)은 무엇인지 정의합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 요약\n",
    "\n",
    "| 파일명 | 비유 | 역할 |\n",
    "| :--- | :--- | :--- |\n",
    "| `config.json` | **설계도** | 모델 구조 정의 |\n",
    "| `model.safetensors` | **뇌** | 학습된 지식(가중치) 저장 |\n",
    "| `tokenizer.json` | **번역기** | 텍스트 ↔ 숫자 변환 |\n",
    "| `generation_config.json` | **말하기 습관** | 생성 옵션 설정 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

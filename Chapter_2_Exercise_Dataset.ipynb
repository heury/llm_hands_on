{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d50cdf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 중요: 이 코드는 GPT 모델 학습을 위한 데이터셋 클래스를 정의합니다\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt (str): 학습할 전체 텍스트 데이터\n",
    "            tokenizer: 텍스트를 토큰 ID로 변환해주는 토크나이저 (예: tiktoken)\n",
    "            max_length (int): 모델이 한 번에 볼 수 있는 윈도우 크기 (입력 시퀀스 길이)\n",
    "            stride (int): 윈도우를 이동시킬 간격 (데이터 중복 정도를 결정)\n",
    "        \"\"\"\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # 1. 전체 텍스트 토큰화\n",
    "        # 텍스트를 정수 리스트(token_ids)로 변환합니다.\n",
    "        # <|endoftext|> 같은 특수 토큰도 허용하여 인코딩합니다.\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # 데이터가 너무 짧으면 학습할 수 없으므로 최소 길이를 확인합니다.\n",
    "        assert len(token_ids) > max_length, \"토큰화된 입력의 개수는 적어도 max_length+1과 같아야 합니다.\"\n",
    "\n",
    "        # 2. 슬라이딩 윈도우(Sliding Window)로 데이터 생성\n",
    "        # 전체 토큰 리스트를 훑으며 max_length 길이만큼 잘라냅니다.\n",
    "        # stride만큼 건너뛰며 반복합니다.\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            \n",
    "            # 입력 청크: 현재 위치(i)부터 max_length만큼 가져옵니다.\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            \n",
    "            # 타겟 청크: 입력보다 1칸 뒤의 위치(i+1)부터 가져옵니다.\n",
    "            # GPT는 '다음 단어'를 맞추는 모델이므로, 정답은 입력보다 한 칸씩 뒤로 밀려있어야 합니다.\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            \n",
    "            # 추출한 데이터를 텐서(Tensor)로 변환하여 리스트에 저장합니다.\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 총 샘플(청크) 개수를 반환합니다.\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # DataLoader가 데이터를 요청할 때 호출됩니다.\n",
    "        # 해당 인덱스(idx)의 입력과 정답 쌍을 반환합니다.\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022bae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "##중요 : 아래 함수는 DataLoader를 생성하는 헬퍼 함수입니다.\n",
    "import tiktoken\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # 토크나이저를 초기화합니다.\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # 데이터셋을 만듭니다.\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # 데이터 로더를 만듭니다.\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf379cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"datas/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f4d74",
   "metadata": {},
   "source": [
    "## Tokenizer 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803265ab",
   "metadata": {},
   "source": [
    "- 아래 결과를 보면 한국어는 Exaone, 중국어는 Qwen의 Tokenizer 성능이 좋습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6de65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# Tokenizer 로드 (로컬 또는 온라인에서)\n",
    "print(\"Tokenizer 로드 중...\")\n",
    "qwen = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "exaone = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-1.2B\")\n",
    "\n",
    "# 테스트할 텍스트\n",
    "texts = [\n",
    "    \"안녕하세요, 반갑습니다!\",\n",
    "    \"Hello, nice to meet you!\",\n",
    "    \"인공지능 기술이 발전하고 있습니다.\",\n",
    "    \"Hello! This is an English tokenizer test.\",\n",
    "    \"こんにちは！日本語のテストです。\",\n",
    "    \"人工智能技术正在快速发展。\",\n",
    "    \"The quick brown fox jumps over the lazy dog. 빠른 갈색 여우가 게으른 개를 뛰어넘습니다.\",\n",
    "    \"파이썬 프로그래밍은 매우 유용합니다. Python programming is very useful.\",\n",
    "    \"123456789 !@#$%^&*() 특수문자\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "for text in texts:\n",
    "    print(f\"\\n텍스트: {text}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Qwen3\n",
    "    qwen_tokens = qwen.encode(text, add_special_tokens=True)\n",
    "    qwen_token_strs = [qwen.decode([tok]) for tok in qwen_tokens[:10]]\n",
    "    print(f\"Qwen3:  {len(qwen_tokens)}개 토큰\")\n",
    "    print(f\"        {qwen_token_strs}\")\n",
    "    \n",
    "    # EXAONE\n",
    "    exaone_tokens = exaone.encode(text, add_special_tokens=True)\n",
    "    exaone_token_strs = [exaone.decode([tok]) for tok in exaone_tokens[:10]]\n",
    "    print(f\"EXAONE: {len(exaone_tokens)}개 토큰\")\n",
    "    print(f\"        {exaone_token_strs}\")\n",
    "    \n",
    "    # 비교\n",
    "    diff = len(qwen_tokens) - len(exaone_tokens)\n",
    "    if diff > 0:\n",
    "        print(f\"→ EXAONE이 {diff}개 더 효율적\")\n",
    "    elif diff < 0:\n",
    "        print(f\"→ Qwen3가 {abs(diff)}개 더 효율적\")\n",
    "    else:\n",
    "        print(f\"→ 동일\")\n",
    "\n",
    "    # Special tokens 비교\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. Special Tokens 비교\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n[Qwen3 Special Tokens]\")\n",
    "for token_name in ['bos_token', 'eos_token', 'unk_token', 'pad_token']:\n",
    "    token = getattr(qwen, token_name, None)\n",
    "    if token:\n",
    "        token_id = getattr(qwen, f\"{token_name}_id\", None)\n",
    "        print(f\"  {token_name}: '{token}' (ID: {token_id})\")\n",
    "\n",
    "print(f\"\\n[EXAONE Special Tokens]\")\n",
    "for token_name in ['bos_token', 'eos_token', 'unk_token', 'pad_token']:\n",
    "    token = getattr(exaone, token_name, None)\n",
    "    if token:\n",
    "        token_id = getattr(exaone, f\"{token_name}_id\", None)\n",
    "        print(f\"  {token_name}: '{token}' (ID: {token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e4365",
   "metadata": {},
   "source": [
    "## Hugging Face 모델 파일 구조 설명\n",
    "\n",
    "Hugging Face Hub에 업로드된 파일들은 **Transformer 기반 언어 모델(LLM)**을 구성하는 핵심 요소들입니다. 크게 **설계도(Config)**, **가중치(Weights)**, **토크나이저(Tokenizer)** 세 그룹으로 나뉩니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 모델 아키텍처 및 설정 (설계도)\n",
    "\n",
    "* **`config.json`**\n",
    "    * **의미:** 모델의 **구조와 설계도**가 담긴 가장 중요한 설정 파일입니다.\n",
    "    * **내용:** 모델의 종류(Llama, BERT 등), 레이어(Layer) 수, 히든 사이즈, 어텐션 헤드 수 등을 정의하여 빈 모델(Skeleton)을 만듭니다.\n",
    "\n",
    "* **`generation_config.json`**\n",
    "    * **의미:** 텍스트 **생성(Generation) 시 사용할 기본 옵션** 파일입니다.\n",
    "    * **내용:** `max_length`(최대 길이), `temperature`(창의성), `top_p`, `eos_token_id`(종료 토큰) 등의 기본 설정값이 들어있습니다.\n",
    "\n",
    "### 2. 모델 가중치 (학습된 뇌)\n",
    "\n",
    "* **`model.safetensors`**\n",
    "    * **의미:** 실제 **학습된 파라미터(가중치)**가 저장된 파일입니다. (가장 용량이 큼)\n",
    "    * **특징:**\n",
    "        * 기존 PyTorch의 `.bin` 파일보다 로딩 속도가 빠릅니다.\n",
    "        * 보안 취약점(악성 코드 실행 등)을 해결한 안전한 형식입니다.\n",
    "\n",
    "### 3. 토크나이저 (언어 처리기)\n",
    "사람의 언어(Text)를 모델이 이해하는 숫자(Token ID)로 변환하는 규칙들입니다.\n",
    "\n",
    "* **`vocab.json`**\n",
    "    * **의미:** **단어장(Vocabulary)** 파일입니다. 단어와 그에 대응하는 숫자 ID가 매핑되어 있습니다.\n",
    "* **`merges.txt`**\n",
    "    * **의미:** **BPE 병합 규칙** 파일입니다. 알파벳 조각들이 어떻게 합쳐져서 단어가 되는지 정의합니다.\n",
    "* **`tokenizer.json`**\n",
    "    * **의미:** `vocab.json`과 `merges.txt` 등을 하나로 통합하여 최적화한 파일로, 로딩 속도가 매우 빠릅니다.\n",
    "* **`tokenizer_config.json`**\n",
    "    * **의미:** 토크나이저 설정 파일입니다. 어떤 클래스를 쓸지, 특수 토큰(`<s>`, `<pad>` 등)은 무엇인지 정의합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 요약\n",
    "\n",
    "| 파일명 | 비유 | 역할 |\n",
    "| :--- | :--- | :--- |\n",
    "| `config.json` | **설계도** | 모델 구조 정의 |\n",
    "| `model.safetensors` | **뇌** | 학습된 지식(가중치) 저장 |\n",
    "| `tokenizer.json` | **번역기** | 텍스트 ↔ 숫자 변환 |\n",
    "| `generation_config.json` | **말하기 습관** | 생성 옵션 설정 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-hands-on",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
